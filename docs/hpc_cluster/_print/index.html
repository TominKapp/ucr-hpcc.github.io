<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.83.1"><link rel=canonical type=text/html href=/docs/hpc_cluster/><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>HPC Cluster | HPCC Home</title><meta property="og:title" content="HPC Cluster"><meta property="og:description" content="What does your user need to know to try your project?
"><meta property="og:type" content="website"><meta property="og:url" content="/docs/hpc_cluster/"><meta property="og:site_name" content="HPCC Home"><meta itemprop=name content="HPC Cluster"><meta itemprop=description content="What does your user need to know to try your project?
"><meta name=twitter:card content="summary"><meta name=twitter:title content="HPC Cluster"><meta name=twitter:description content="What does your user need to know to try your project?
"><link rel=preload href=/scss/main.min.e8d637f30893e85918b2aeae9cd9eab6c68fcd7998810e804d7c9ab542fbfd45.css as=style><link href=/scss/main.min.e8d637f30893e85918b2aeae9cd9eab6c68fcd7998810e804d7c9ab542fbfd45.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg></span><span class="text-uppercase font-weight-bold">HPCC Home</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/access-rates/access-rates/><span>Access & Rates</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/contacts/staff/><span>Facility Staff & Contacts</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/contacts/location/><span>Location</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/docs/><span class=active>Documentation</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete=off></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/hpc_cluster/>Return to the regular view of this page</a>.</p></div><h1 class=title>HPC Cluster</h1><div class=lead>What does your user need to know to try your project?</div><ul><li>1: <a href=#pg-11b484466b67942fb48624c86d4e7516>Communicating</a></li><li>2: <a href=#pg-cd50af3ad492702e9c81b41ea41ca71d>Data Storage</a></li><li>3: <a href=#pg-2b49eeb920221ab59fb8d5d729919a03>Getting Started</a></li><li>4: <a href=#pg-7a302e67293d2d71459df538011c1425>Introduction</a></li><li>5: <a href=#pg-0015a1dfd4c54db6027a877e90fd94f7>Login</a></li><li>6: <a href=#pg-db94df5798125d6b05102f1dcc47ef7c>Managing Jobs</a></li><li>7: <a href=#pg-87f5b7d632f7c377465149375e5ab359>Package Management</a></li><li>8: <a href=#pg-2da4a1c0ba6afe09bb67a3b123aee2f9>Parallel Evaluations in R</a></li><li>9: <a href=#pg-dcbb75e455948c3241cb60aaa145d06c>Queue Policies</a></li><li>10: <a href=#pg-5f2edcb7e51abae3e3abc6d5929e6ded>Security</a></li><li>11: <a href=#pg-a3377e68a8e7a29ab44c288bf6043b45>Sharing Data</a></li><li>12: <a href=#pg-96d8c142fd8b2269c2f97cff55ef365f>SSH Keys Apple macOS</a></li><li>13: <a href=#pg-a7ae8375cc7ee8eeaa194b633a0ec206>SSH Keys Microsoft Windows</a></li><li>14: <a href=#pg-6ac662661d338690e787bb2d160108ef>Terminal-based Working Environments</a></li><li>15: <a href=#pg-25afe2f58c1a6825ea3c6ea9a5dc3fe3>Visualization</a></li></ul><div class=content><div class="pageinfo pageinfo-primary"><p>This is a placeholder page that shows you how to use this template site.</p></div><p>Information in this section helps your user try your project themselves.</p><ul><li><p>What do your users need to do to start using your project? This could include downloading/installation instructions, including any prerequisites or system requirements.</p></li><li><p>Introductory “Hello World” example, if appropriate. More complex tutorials should live in the Tutorials section.</p></li></ul><p>Consider using the headings below for your getting started page. You can delete any that are not applicable to your project.</p><h2 id=prerequisites>Prerequisites</h2><p>Are there any system requirements for using your project? What languages are supported (if any)? Do users need to already have any software or tools installed?</p><h2 id=installation>Installation</h2><p>Where can your user find your project code? How can they install it (binaries, installable package, build from source)? Are there multiple options/versions they can install and how should they choose the right one for them?</p><h2 id=setup>Setup</h2><p>Is there any initial setup users need to do after installation to try your project?</p><h2 id=try-it-out>Try it out!</h2><p>Can your users test their installation, for example by running a command or deploying a Hello World example?</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-11b484466b67942fb48624c86d4e7516>1 - Communicating</h1><h2 id=communicating-with-others>Communicating with others</h2><p>The cluster is a shared resource, and communicating with other users can help to schedule large computations.</p><p><strong>Looking-Up Specific Users</strong></p><p>A convenient overview of all users and their lab affiliations can be retrieved with the following command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>user_details.sh
</code></pre></div><p>You can search for specific users by running:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#000>MATCH1</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#39;username1&#39;</span> <span style=color:#8f5902;font-style:italic># Searches by real name, and username, and email address and PI name</span>
<span style=color:#000>MATCH2</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#39;username2&#39;</span>
user_details.sh <span style=color:#000;font-weight:700>|</span> grep -P <span style=color:#4e9a06>&#34;</span><span style=color:#000>$MATCH1</span><span style=color:#4e9a06>|</span><span style=color:#000>$MATCH2</span><span style=color:#4e9a06>&#34;</span>
</code></pre></div><p><strong>Listing Users with Active Jobs on the Cluster</strong>
To get a list of usernames:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>squeue --format <span style=color:#4e9a06>&#39;%u&#39;</span> <span style=color:#000;font-weight:700>|</span> sort <span style=color:#000;font-weight:700>|</span> uniq
</code></pre></div><p>To get the list of real names:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>grep &lt;<span style=color:#ce5c00;font-weight:700>(</span>user_details.sh <span style=color:#000;font-weight:700>|</span> awk <span style=color:#4e9a06>&#39;{print $2,$3,$4}&#39;</span><span style=color:#ce5c00;font-weight:700>)</span> -f &lt;<span style=color:#ce5c00;font-weight:700>(</span>squeue --format <span style=color:#4e9a06>&#39;%u&#39;</span> --noheader <span style=color:#000;font-weight:700>|</span> sort <span style=color:#000;font-weight:700>|</span> uniq<span style=color:#ce5c00;font-weight:700>)</span> <span style=color:#000;font-weight:700>|</span> awk <span style=color:#4e9a06>&#39;{print $1,$2}&#39;</span>
</code></pre></div><p>To get the list of emails:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>grep &lt;<span style=color:#ce5c00;font-weight:700>(</span>user_details.sh <span style=color:#000;font-weight:700>|</span> awk <span style=color:#4e9a06>&#39;{print $4,$5}&#39;</span><span style=color:#ce5c00;font-weight:700>)</span> -f &lt;<span style=color:#ce5c00;font-weight:700>(</span>squeue --format <span style=color:#4e9a06>&#39;%u&#39;</span> --noheader <span style=color:#000;font-weight:700>|</span> sort <span style=color:#000;font-weight:700>|</span> uniq<span style=color:#ce5c00;font-weight:700>)</span> <span style=color:#000;font-weight:700>|</span> awk <span style=color:#4e9a06>&#39;{print $2}&#39;</span>
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-cd50af3ad492702e9c81b41ea41ca71d>2 - Data Storage</h1><h2 id=dashboard>Dashboard</h2><p>HPCC cluster users are able to check on their home and bigdata storage usage from the <a href=https://dashboard.hpcc.ucr.edu>Dashboard Portal</a>.</p><h2 id=home>Home</h2><p>Home directories are where you start each session on the HPC cluster and where your jobs start when running on the cluster. This is usually where you place the scripts and various things you are working on. This space is very limited. Please remember that the home storage space quota per user account is 20 GB.</p><table><thead><tr><th>Path</th><th>/rhome/<code>username</code></th></tr></thead><tbody><tr><td>User Availability</td><td>All Users</td></tr><tr><td>Node Availability</td><td>All Nodes</td></tr><tr><td>Quota Responsibility</td><td>User</td></tr></tbody></table><h2 id=bigdata>Bigdata</h2><p>Bigdata is an area where large amounts of storage can be made available to users. A lab purchases bigdata space separately from access to the cluster. This space is then made available to the lab via a shared directory and individual directories for each user.</p><p><strong>Lab Shared Space</strong>
This directory can be accessed by the lab as a whole.</p><table><thead><tr><th>Path</th><th>/bigdata/<code>labname</code>/shared</th></tr></thead><tbody><tr><td>User Availability</td><td>Labs that have purchased space.</td></tr><tr><td>Node Availability</td><td>All Nodes</td></tr><tr><td>Quota Responsibility</td><td>Lab</td></tr></tbody></table><p><strong>Individual User Space</strong>
This directory can be accessed by specific lab members.</p><table><thead><tr><th>Path</th><th>/bigdata/<code>labname</code>/<code>username</code></th></tr></thead><tbody><tr><td>User Availability</td><td>Labs that have purchased space.</td></tr><tr><td>Node Availability</td><td>All Nodes</td></tr><tr><td>Quota Responsibility</td><td>Lab</td></tr></tbody></table><h2 id=non-persistent-space>Non-Persistent Space</h2><p>Frequently, there is a need to do things like, output a significant amount of intermediate data during a job, access a dataset from a faster medium than bigdata or the home directories or write out lock files. These types of things are well suited to the use of non-persistent spaces. Below are the filesystems available on the HPC cluster.</p><p><strong>Temporary Space</strong>
This is a standard space available on all Linux systems. Please be aware that it is limited to the amount of free disk space on the node you are running on.</p><table><thead><tr><th>Path</th><th>/tmp</th></tr></thead><tbody><tr><td>User Availability</td><td>All Users</td></tr><tr><td>Node Availability</td><td>All Nodes</td></tr><tr><td>Quota Responsibility</td><td>N/A</td></tr></tbody></table><p><strong>SSD Backed Space</strong>
This space is much faster than the persistwnt space (/rhome,/bigdata), but slower than using RAM based storage.</p><table><thead><tr><th>Path</th><th>/scratch</th></tr></thead><tbody><tr><td>User Availability</td><td>All Users</td></tr><tr><td>Node Availability</td><td>All Nodes</td></tr><tr><td>Quota Responsibility</td><td>N/A</td></tr></tbody></table><p><strong>RAM Space</strong>
This type of space takes away from physical memory but allows extremely fast access to the files located on it. When submitting a job you will need to factor in the space your job is using in RAM as well. For example, if you have a dataset that is 1G in size and use this space, it will take at least 1G of RAM.</p><table><thead><tr><th>Path</th><th>/dev/shm</th></tr></thead><tbody><tr><td>User Availability</td><td>All Users</td></tr><tr><td>Node Availability</td><td>All Nodes</td></tr><tr><td>Quota Responsibility</td><td>N/A</td></tr></tbody></table><h2 id=usage-and-quotas>Usage and Quotas</h2><p>To quickly check your usage and quota limits:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>check_quota home
check_quota bigdata
</code></pre></div><p>To get the usage of your current directory, run the following command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>du -sh .
</code></pre></div><p>To calculate the sizes of each separate sub directory, run:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>du -shc *
</code></pre></div><p>This may take some time to complete, please be patient.</p><p>For more information on your home directory, please see the <a href=manuals_linux-basics_cmdline-basics.html#orientation>Linux Basics Orientation</a>.</p><h2 id=automatic-backups>Automatic Backups</h2><p>The cluster does create backups however it is still advantageous for users to periodically make copies of their critical data to a separate storage device.
The cluster is a production system for research computations with a very expensive high-performance storage infrastructure. It is not a data archiving system.</p><p>Home backups are created daily and kept for one week.
Bigdata backups are created weekly and kept for one month.</p><p>Home and bigdata backups are located under the following respective directories:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>/rhome/.snapshots/
/bigdata/.snapshots/
</code></pre></div><p>The individual snapshot directories have names with numerical values in epoch time format.
The higher the value the more recent the snapshot.</p><p>To view the exact time of when each snapshot was taken execute the following commands:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>mmlssnapshot home
mmlssnapshot bigdata
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2b49eeb920221ab59fb8d5d729919a03>3 - Getting Started</h1><h2 id=login-from-mac-linux-mobaxterm>Login from Mac, Linux, MobaXTerm</h2><p>The initial login brings users into the cluster head node (i.e. pigeon, penguin, owl). From there, users can submit jobs via qsub to the compute nodes or log into owl to perform intensive tests.
Since all machines are mounting a centralized file system, users will always see the same home directory on all systems. Therefore, there is no need to copy files from one machine to another.</p><p>Open the terminal and type</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh -X username@cluster.hpcc.ucr.edu
</code></pre></div><h2 id=login-from-windows>Login from Windows</h2><p>Please refer to the login instructions of our <a href=manuals_linux-basics_intro#windows>Linux Basics manual</a>.</p><h2 id=change-password>Change Password</h2><ol><li>Login via SSH using the Terminal on Mac/Linux or MobaXTerm on Windows</li></ol><ul><li>Once you have logged in type the following command:</li></ul><pre><code>passwd
</code></pre><ul><li>Enter the old password (the random characters that you were given as your initial password)</li><li>Enter your new password</li></ul><p>The password minimum requirements are:</p><ul><li>Total length at least 8 characters long</li><li>Must have at least 3 of the following:<ul><li>Lowercase character</li><li>Uppercase character</li><li>Number</li><li>Punctuation character</li></ul></li></ul><h2 id=modules>Modules</h2><p>All software used on the HPC cluster is managed through a simple module system.
You must explicitly load and unload each package as needed.
More advanced users may want to load modules within their bashrc, bash_profile, or profile files.</p><h3 id=available-modules>Available Modules</h3><p>To list all available software modules, execute the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>module avail
</code></pre></div><p>This should output something like:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>------------------------- /usr/local/Modules/versions --------------------------
3.2.9
--------------------- /usr/local/Modules/3.2.9/modulefiles ---------------------
BEDTools/2.15.0<span style=color:#ce5c00;font-weight:700>(</span>default<span style=color:#ce5c00;font-weight:700>)</span> modules
PeakSeq/1.1<span style=color:#ce5c00;font-weight:700>(</span>default<span style=color:#ce5c00;font-weight:700>)</span> python/3.2.2
SOAP2/2.21<span style=color:#ce5c00;font-weight:700>(</span>default<span style=color:#ce5c00;font-weight:700>)</span> samtools/0.1.18<span style=color:#ce5c00;font-weight:700>(</span>default<span style=color:#ce5c00;font-weight:700>)</span>
bowtie2/2.0.0-beta5<span style=color:#ce5c00;font-weight:700>(</span>default<span style=color:#ce5c00;font-weight:700>)</span> stajichlab
cufflinks/1.3.0<span style=color:#ce5c00;font-weight:700>(</span>default<span style=color:#ce5c00;font-weight:700>)</span> subread/1.1.3<span style=color:#ce5c00;font-weight:700>(</span>default<span style=color:#ce5c00;font-weight:700>)</span>
matrix2png/1.2.1<span style=color:#ce5c00;font-weight:700>(</span>default<span style=color:#ce5c00;font-weight:700>)</span> tophat/1.4.1<span style=color:#ce5c00;font-weight:700>(</span>default<span style=color:#ce5c00;font-weight:700>)</span>
module-info
</code></pre></div><h3 id=using-modules>Using Modules</h3><p>To load a module, run:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>module load &lt;software name&gt;<span style=color:#ce5c00;font-weight:700>[</span>/&lt;version&gt;<span style=color:#ce5c00;font-weight:700>]</span>
</code></pre></div><p>To load the default version of the tophat module, run:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>module load tophat
</code></pre></div><h3 id=show-loaded-modules>Show Loaded Modules</h3><p>To show what modules you have loaded at any time, you can run:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>module list
</code></pre></div><p>Depending on what modules you have loaded, it will produce something like this:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>Currently Loaded Modulefiles:
  1<span style=color:#ce5c00;font-weight:700>)</span> vim/7.4.1952                  3<span style=color:#ce5c00;font-weight:700>)</span> slurm/16.05.4                 5<span style=color:#ce5c00;font-weight:700>)</span> R/3.3.0                       7<span style=color:#ce5c00;font-weight:700>)</span> less-highlight/1.0            9<span style=color:#ce5c00;font-weight:700>)</span> python/3.6.0
  2<span style=color:#ce5c00;font-weight:700>)</span> tmux/2.2                      4<span style=color:#ce5c00;font-weight:700>)</span> openmpi/2.0.1-slurm-16.05.4   6<span style=color:#ce5c00;font-weight:700>)</span> perl/5.20.2                   8<span style=color:#ce5c00;font-weight:700>)</span> iigb_utilities/1
</code></pre></div><h3 id=unloading-software>Unloading Software</h3><p>Sometimes you want to no longer have a piece of software in path. To do this you unload the module by running:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>module unload &lt;software name&gt;
</code></pre></div><h2 id=databases>Databases</h2><h3 id=loading-databases>Loading Databases</h3><p><a href=http://www.ncbi.nlm.nih.gov/>NCBI</a>, <a href=http://en.wikipedia.org/wiki/Pfam#External_links>PFAM</a>, and <a href=http://www.uniprot.org/>Uniprot</a>, do not need to be downloaded by users. They are installed as modules on the cluster.</p><pre><code>module load db-ncbi
module load db-pfam
module load db-uniprot
</code></pre><p>Specific database release numbers can be identified by the version label on the module:</p><pre><code>module avail db-ncbi

----------------- /usr/local/Modules/3.2.9/modulefiles -----------------
db-ncbi/20140623(default)
</code></pre><h3 id=using-databases>Using Databases</h3><p>In order to use the loaded database users can simply provide the corresponding environment variable (NCBI_DB, UNIPROT_DB, PFAM_DB, etc&mldr;) for the proper path in their executables.</p><p>This is the old deprecated BLAST and it may not work in the near future, however if you require it:</p><pre><code>blastall -p blastp -i proteins.fasta -d $NCBI_DB/nr -o blastp.out
</code></pre><p>You can can also use this method if you require the old version of BLAST (old BLAST with legacy support):</p><pre><code>BLASTBIN=`which legacy_blast.pl | xargs dirname`
legacy_blast.pl blastall -p blastp -i proteins.fasta -d $NCBI_DB/nr -o blast.out --path $BLASTBIN
</code></pre><p>This is the preferred/recommended method (BLAST+):</p><pre><code>blastp -query proteins.fasta -db $NCBI_DB/nr -out proteins_blastp.txt
</code></pre><p>Usually, we store the most recent release and 2-3 previous releases of each database. This way time consuming projects can use the same database version throughout their lifetime without always updating to the latest releases.</p><h3 id=additional-features>Additional Features</h3><p>There are additional features and operations that can be done with the module command. Please run the following to get more information:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>module <span style=color:#204a87>help</span>
</code></pre></div><h2 id=quotas>Quotas</h2><h3 id=cpu>CPU</h3><p>Currently, the maximum number of CPU cores a user can use simultaneously on the cluster is 256 CPU cores when the load on the cluster is &lt;30% and 128 CPU cores when the load is above 30%. If a user submits jobs for more than 256/128 CPU cores then the additional requests will be queued until resources within the user&rsquo;s CPU quota become available. Upon request a user&rsquo;s upper CPU quota can be extended temporarily, but only if sufficient CPU resources are available. To avoid monopolisation of the cluster by a small number of users, the high load CPU quota of 128 cores is dynamically readjusted by an algorithm that considers the number of CPU hours accumulated by each user over a period of 2 weeks along with the current overall CPU usage on the cluster. If the CPU hour average over the 2 week window exceeds an allowable amount then the default CPU quota will be reduced for such a heavy user to 64 CPU cores, and if it exceeds the allowable amount by two-fold it will be reduced to 32 CPU cores. Once the average usage of a heavy user drops again below those limits, the upper CPU limit will be raised accordingly. Note: when the overall CPU load on the cluster is below 70% then the dynamically readjusted CPU quotas are not applied. At those low load times every user has the same CPU quota: 256 CPU cores at &lt;30% load and 128 CPU cores at 30-70% load.</p><h3 id=data-storage>Data Storage</h3><p>A standard user account has a storage quota of 20GB. Much more storage space, in the range of many TBs, can be made available in a user account&rsquo;s bigdata directory. The amount of storage space available in bigdata depends on a user group&rsquo;s annual subscription. The pricing for extending the storage space in the bigdata directory is available <a href=/home>here</a>.</p><h3 id=memory>Memory</h3><p>From the cluster head node users can submit jobs to the batch queue or the highmem queue. The nodes associated with the batch queue are mainly for CPU intensive tasks, while the nodes of the highmem queue are dedicated to memory intensive tasks. The batch nodes allow a 1GB RAM minimum limit on jobs and and the highmem nodes allow 100GB-1024GB RAM jobs.</p><h2 id=whats-next>What&rsquo;s Next?</h2><p>You should now know the following:</p><ol><li>Basic orginization of the cluster</li></ol><ul><li>How to login to the cluster</li><li>How to use the Module system to gain access to the cluster software</li><li>CPU, storage, and memory limitations (quotas and hardware limits)</li></ul><p>Now you can start using the cluster.
The recommended way to run your jobs (scripts, pipelines, experiments, etc&mldr;) is to submit them to the queuing system by using sbatch.
The HPCC cluster uses the Slurm queuing system.
Please do not run ANY computationally intensive tasks on any head node that starts with the letter &ldquo;P&rdquo; (i.e. penguin, pigeon, parrot). If this policy is violated, your jobs will be killed to limit the negative impact on others.
The head nodes are a shared resource and should be accessible by all users. Negatively impacting performance would affect all users on the system and will not be tolerated.</p><p>However you may run memory intensive jobs on Owl.
Login to Owl like so:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh -X owl.ucr.edu
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-7a302e67293d2d71459df538011c1425>4 - Introduction</h1><h2 id=introduction>Introduction</h2><p>This manual provides an introduction to the usage of the HPCC cluster.
All servers and compute resources of the HPCC cluster are available to researchers from all departments and colleges at UC Riverside for a minimal recharge fee <a href=/#rates>(see rates)</a>.
To request an account, please email <a href=mailto:support@hpcc.ucr.edu>support@hpcc.ucr.edu</a>.
The latest hardware/facility description for grant applications is available <a href=https://goo.gl/43eOwQ>here</a>.</p><h2 id=overview>Overview</h2><h3 id=storage>Storage</h3><ul><li>Four enterprise class HPC storage systems</li><li>Approximately 2 PB (2048 TB) of network storage</li><li>GPFS (NFS and SAMBA via GPFS)</li><li>Automatic snapshots and archival backups</li></ul><h3 id=network>Network</h3><ul><li>Ethernet<ul><li>1 Gb/s switch x 5</li><li>1 Gb/s switch 10 Gig uplink</li><li>10 Gb/s switch for Campus wide Science DMZ</li><li>redundant, load balanced, robust mesh topology</li></ul></li><li>Interconnect<ul><li>56 Gb/s InfiniBand (FDR)</li></ul></li></ul><h3 id=head-nodes>Head Nodes</h3><p>All users should access the cluster via ssh through cluster.hpcc.ucr.edu, this address will automatically balance traffic to one of the available head nodes.</p><ul><li>Penguin<ul><li>Resources: 8 cores, 64 GB memory</li><li>Primary function: submitting jobs to the queuing system</li><li>Secondary function: development; code editing and running small (under 50 % CPU and under 1 GB RAM) sample jobs</li></ul></li><li>Pigeon<ul><li>Resources: 16 cores, 128 GB memory</li><li>Primary function: submitting jobs to the queuing system</li><li>Secondary function: development; code editing and running small (under 50 % CPU and under 1 GB RAM) sample jobs</li></ul></li><li>Pelican<ul><li>Resources: 32 cores, 64 GB memory</li><li>Primary function: submitting jobs to the queuing system</li><li>Secondary function: development; code editing and running small (under 50 % CPU and under 1 GB RAM) sample jobs</li></ul></li><li>Owl<ul><li>Resources: 16 cores, 64 GB memory</li><li>Primary function: testing; running test sets of jobs</li><li>Secondary function: submitting jobs to the queuing system</li></ul></li><li>Globus<ul><li>Resources: 32 cores, 32 GB memory</li><li>Primary function: submitting jobs to the queuing system</li><li>Secondary function: development; code editing and running small (under 50 % CPU and under 1 GB RAM) sample jobs</li></ul></li></ul><h3 id=worker-nodes>Worker Nodes</h3><ul><li>Batch<ul><li>c01-c48: each with 64 AMD cores and 512 GB memory</li></ul></li><li>Highmem<ul><li>h01-h06: each with 32 Intel cores and 1024 GB memory</li></ul></li><li>GPU<ul><li>gpu01-gpu02: each with 32 (HT) cores Intel Haswell CPUs and 2 x NVIDIA Tesla K80 GPUs (~10000 CUDA cores each) and 128 GB memory</li><li>gpu03-gpu04: each with 32 (HT) cores Intel Haswell CPUs and 4 x NVIDIA Tesla K80 GPUs (~10000 CUDA cores each) and 128 GB memory</li></ul></li><li>Intel<ul><li>i01-i40: each with 32 Intel Broadwell cores and 512 GB memory</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0015a1dfd4c54db6027a877e90fd94f7>5 - Login</h1><h2 id=login>Login</h2><p>We are moving all accounts to a more secure method of authentication for logging into the cluster.
Passwords alone will no longer be allowed, but rather <a href=#passwordduo>Password+DUO</a> or <a href=#ssh-keys>SSH Keys</a>.</p><p>Roll-Out Plan:</p><ol><li>Old (password) and new (secure) authentication methods are provided through <code>cluster.hpcc.ucr.edu</code> and <code>secure.hpcc.ucr.edu</code> respectively.</li><li>Users configure new authentication method.</li><li>Users log into the cluster using host <code>secure.hpcc.ucr.edu</code>.</li><li>After the authentication switch over deadline (TBD), host <code>cluster.hpcc.ucr.edu</code> switches over to new (secure) authentication methods. The old (password) authentication method is completely deprecated.</li></ol><h2 id=secure-authentication>Secure Authentication</h2><p>There are two methods of authentication that the cluster supports:</p><ol><li><a href=#passwordduo>Password+DUO</a></li><li><a href=#ssh-keys>SSH Keys</a></li></ol><h3 id=passwordduo>Password+Duo</h3><p>The <code>Password+DUO</code> combination method will only work if your UCR NetID matches your cluster username.
If these two match then first check if you already have DUO installed and configured on a mobile device.
If you already have used DUO with other UCR campus multi-factor enabled sites or utilites, great!
Otherwise, if you have not yet installed, nor configured DUO on a mobile device, then you will need to do so by enrolling:
<a href=https://cnc.ucr.edu/mfa/enrollment.html>https://cnc.ucr.edu/mfa/enrollment.html</a></p><p>Once you have DUO installed and configured on your mobile device, then retrieve your password for the cluster.
If you have a new account then your password was emailed to you when your account was created.</p><p>In order to test this try to log into the cluster through the <code>secure</code> server:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh username@secure.hpcc.ucr.edu
</code></pre></div><p>Remember to replace <code>username</code> with your real cluster username, which should also match your UCR NetID.</p><p>Assuming that you have already installed and configured DUO on a mobile device, then when attempting to login you will be first asked to provide your password, and then you will need to choose your DUO authentication option to validate your login attempt.
Depending on how you chose to configure/enrolling your mobile device, you may see multiple options.</p><p>DUO uses either an option for DUO authentication via <code>Push</code> which uses the mobile app, or via <code>SMS</code> which sends a code as a text message to your phone.
Choose whichever option works best for you.</p><p>After logging in successfully, you are expected to update your password with the <code>passwd</code> command.</p><p>For more general information regarding Multi-Factor Authentication and DUO, please visit the following:
<a href=https://cnc.ucr.edu/mfa/how.html>https://cnc.ucr.edu/mfa/how.html</a></p><h3 id=ssh-keys>SSH Keys</h3><p>SSH keys can only be setup if you already have access to the cluster.
This is becuase in order to get this working a file needs to be placed in your home directory on the cluster.</p><p>When using SSH key authentication, you will need to create a public and a pritate key.
This is analogous to how a key and a lock are used in the real world, one uniquely fits to the other.
Only when your private key &ldquo;fits&rdquo; the public key, can you be granted access.</p><p>To create the key pair run the following command on your computer (<a href=manuals_linux-basics_intro#mac>Terminal</a>/<a href=manuals_linux-basics_intro#windows>MobaXterm</a>):</p><pre><code># Create SSH directory
mkdir -p ~/.ssh

# Create key pair (Private and Public)
ssh-keygen -t rsa -f ~/.ssh/id_rsa
</code></pre><p>Once the command has completed, you will find two files in your <code>~/.ssh</code> directory.</p><pre><code># List files in SSH directory
ls ~/.ssh/
  id_rsa
  id_rsa.pub
</code></pre><p>The <code>id_rsa</code> file is your private key and the <code>id_rsa.pub</code> is your public key.
You will need to copy your public key to the cluster, creating the <code>authorized_keys</code> file.</p><p>From your computer (<a href=manuals_linux-basics_intro#mac>Terminal</a>/<a href=manuals_linux-basics_intro#windows>MobaXterm</a>) run the following:</p><pre><code>scp .ssh/id_rsa.pub username@cluster.hpcc.ucr.edu:.ssh/authorized_keys
</code></pre><p>If the <code>authorized_keys</code> file already exists, you can just append your new public key, like so:</p><pre><code>scp .ssh/id_rsa.pub username@cluster.hpcc.ucr.edu:tmpkey &amp;&amp; ssh username@cluster.hpcc.ucr.edu &quot;cat tmpkey &gt;&gt; ~/.ssh/authorized_keys &amp;&amp; rm tmpkey&quot;
</code></pre><p>In order to test this try to log into the cluster through the <code>secure</code> server:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh username@secure.hpcc.ucr.edu
</code></pre></div><p>Remember to replace <code>username</code> with your real cluster username, which should also match your UCR NetID.</p><blockquote><p>Note: MS Windows (MobaXterm) can also use the graphical SSH keys manager &ldquo;MobaKeyGen&rdquo; (from the &ldquo;Tools&rdquo; menu).</p></blockquote><h2 id=file-transfers>File Transfers</h2><p>We support <code>FileZilla</code> as the recommended graphical file transfer application. If you are comfortable with the command line that is typically easier to use.
However, there may be times when selecting multiple files from a graphical application is prefered.</p><p>When using <code>FileZilla</code> you must create a new site, just click <code>File -> Site Manager</code>.
From the new window click <code>New Site</code>.</p><p>On the right pane fill in the information as follows:</p><pre><code>Protocol    SFTP - SSH File Transfer Protocol
Host        secure.hpcc.ucr.edu
Port        22
</code></pre><p>The <code>Logon Type</code> can be either <code>Interactive</code> or <code>Key File</code>, this depends on if you have setup <a href=#passwordduo>Password+DUO</a> or <a href=#ssh-keys>SSH Keys</a> respectively.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-db94df5798125d6b05102f1dcc47ef7c>6 - Managing Jobs</h1><h2 id=what-is-a-job>What is a Job?</h2><p>Submitting and managing jobs is at the heart of using the cluster. A &lsquo;job&rsquo; refers to the script, pipeline or experiment that you run on the nodes in the cluster.</p><h2 id=partitions>Partitions</h2><p>In the past we used queues under the old Torque system, we now refer to these logically grouped nodes as partitions. There are several different partitions available for cluster users to send jobs to:</p><ul><li>intel<ul><li>Default partition</li><li>Nodes: i01-02,i17-i40</li><li>Cores: Intel, 256 per user</li><li>RAM: 1 GB default</li><li>Time (walltime): 168 hours (7 days) default</li></ul></li><li>batch<ul><li>Nodes: c01-c48</li><li>Cores: AMD, 256 per user</li><li>RAM: 1 GB default</li><li>Time (walltime): 168 hours (7 days) default</li></ul></li><li>highmem<ul><li>Nodes: h01-h06</li><li>Cores: Intel, 32 per user</li><li>RAM: 100 GB min and 1000 GB max</li><li>Time (walltime): 48 hours (2 days) default</li></ul></li><li>gpu<ul><li>Nodes: gpu01-gpu05</li><li>GPUs: 8 per group</li><li>RAM: 1 GB default</li><li>Time (walltime): 48 hours (2 days) default</li></ul></li><li>short<ul><li>Nodes: Mixed set of nodes from batch, intel, and group partitions</li><li>Cores: AMD/Intel, 256 per user</li><li>RAM: 1 GB default</li><li>Time (walltime): 2 hours Maximum</li></ul></li><li>Group Partition<ul><li>This partition is unique to the group, if your lab has purchased nodes then you will have a priority partition with the same name as your group (ie. girkelab).
In order to submit a job to different partitions add the optional &lsquo;-p&rsquo; parameter with the name of the partition you want to use:</li></ul></li></ul><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch -p batch SBATCH_SCRIPT.sh
sbatch -p highmem SBATCH_SCRIPT.sh
sbatch -p gpu SBATCH_SCRIPT.sh
sbatch -p intel SBATCH_SCRIPT.sh
sbatch -p mygroup SBATCH_SCRIPT.sh
</code></pre></div><h2 id=slurm>Slurm</h2><p>Slurm is now our default queuing system across all head nodes. <a href=#getting-started>SSH directly into the cluster</a> and your connection will be automatically load balanced to a head node:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh -XY cluster.hpcc.ucr.edu
</code></pre></div><h3 id=resources-and-limits>Resources and Limits</h3><p>To see your limits you can do the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>slurm_limits
</code></pre></div><p>Check total number of cores used by your group in the all partitions:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>group_cpus
</code></pre></div><p>However this does not tell you when your job will start, since it depends on the duration of each job.
The best way to do this is with the &ldquo;&ndash;start&rdquo; flag on the squeue command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>squeue --start -u <span style=color:#000>$USER</span>
</code></pre></div><h3 id=submitting-jobs>Submitting Jobs</h3><p>There are 2 basic ways to submit jobs; non-interactive, interactive. Slurm will automatically start within the directory where you submitted the job from, so keep that in mind when you use relative file paths.
Non-interactive submission of a SBATCH script:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch SBATCH_SCRIPT.sh
</code></pre></div><p>Here is an example of an SBATCH script:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic>#!/bin/bash -l
</span><span style=color:#8f5902;font-style:italic></span>
<span style=color:#8f5902;font-style:italic>#SBATCH --nodes=1</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --ntasks=1</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=10</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --mem=10G</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --time=1-00:15:00     # 1 day and 15 minutes</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --mail-user=useremail@address.com</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --mail-type=ALL</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --job-name=&#34;just_a_test&#34;</span>
<span style=color:#8f5902;font-style:italic>#SBATCH -p intel # This is the default partition, you can use any of the following; intel, batch, highmem, gpu</span>

<span style=color:#8f5902;font-style:italic># Print current date</span>
date

<span style=color:#8f5902;font-style:italic># Load samtools</span>
module load samtools

<span style=color:#8f5902;font-style:italic># Concatenate BAMs</span>
samtools cat -h header.sam -o out.bam in1.bam in2.bam

<span style=color:#8f5902;font-style:italic># Print name of node</span>
hostname
</code></pre></div><p>The above job will request 1 node, 10 cores (parallel threads), 10GB of memory, for 1 day and 15 minutes. An email will be sent to the user when the status of the job changes (Start, Failed, Completed).
For more information regarding parallel/multi core jobs refer to <a href=#parallelization>Parallelization</a>.</p><p>Interactive submission:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun --pty bash -l
</code></pre></div><p>If you do not specify a partition then the intel partition is used by default.</p><p>Here is a more complete example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun --x11 --mem<span style=color:#ce5c00;font-weight:700>=</span>1gb --cpus-per-task <span style=color:#0000cf;font-weight:700>1</span> --ntasks <span style=color:#0000cf;font-weight:700>1</span> --time 10:00:00 --pty bash -l
</code></pre></div><p>The above example enables X11 forwarding and requests, 1GB of memory, 1 cores, for 10 hours within an interactive session.</p><h3 id=monitoring-jobs>Monitoring Jobs</h3><p>To check on your jobs states, run the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>squeue -u <span style=color:#000>$USER</span> --start
</code></pre></div><p>To list all the details of a specific job, run the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scontrol show job JOBID
</code></pre></div><p>To view past jobs and their details, run the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sacct -u <span style=color:#000>$USER</span> -l
</code></pre></div><p>You can also adjust the start <code>-S</code> time and/or end <code>-E</code> time to view, using the YYYY-MM-DD format.
For example, the following command uses start and end times:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sacct -u <span style=color:#000>$USER</span> -S 2018-01-01 -E 2018-08-30 -l <span style=color:#000;font-weight:700>|</span> less -S <span style=color:#8f5902;font-style:italic># Type &#39;q&#39; to quit</span>
</code></pre></div><h3 id=canceling-jobs>Canceling Jobs</h3><p>In cancel/stop your job run the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scancel &lt;JOBID&gt;
</code></pre></div><p>You can also cancel multiple jobs:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scancel &lt;JOBID1&gt; &lt;JOBID2&gt; &lt;JOBID3&gt;
</code></pre></div><p>If you want to cancel/stop/kill ALL your jobs it is possible with the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic># Be very careful when running this, it will kill all your jobs.</span>
squeue --user <span style=color:#000>$USER</span> --noheader --format <span style=color:#4e9a06>&#39;%i&#39;</span> <span style=color:#000;font-weight:700>|</span> xargs scancel
</code></pre></div><p>For more information please refer to <a href=https://slurm.schedmd.com/scancel.html title="Slurm scancel doc">Slurm scancel documentation</a>.</p><h3 id=advanced-jobs>Advanced Jobs</h3><p>There is a third way of submitting jobs by using steps.
Single Step submission:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun &lt;command&gt;
</code></pre></div><p>Under a single step job your command will hang until appropriate resources are found and when the step command is finished the results will be sent back on STDOUT. This may take some time depending on the job load of the cluster.
Multi Step submission:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>salloc -N <span style=color:#0000cf;font-weight:700>4</span> bash -l
srun &lt;command&gt;
...
srun &lt;command&gt;
<span style=color:#204a87>exit</span>
</code></pre></div><p>Under a multi step job the salloc command will request resources and then your parent shell will be running on the head node. This means that all commands will be executed on the head node unless preceeded by the srun command. You will also need to exit this shell in order to terminate your job.</p><h3 id=highmem-jobs>Highmem Jobs</h3><p>The highmem partition does not have a default amount of memory set, however it does has a minimum limit of 100GB per job. This means that you need to explicity request at least 100GB or more of memory.</p><p>Non-Interactive:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch -p highmem --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>24:00:00 SBATCH_SCRIPT.sh
</code></pre></div><p>Interactive</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun -p highmem --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>24:00:00 --pty bash -l
</code></pre></div><p>Of course you should adjust the time argument according to your job requirements.</p><h3 id=gpu-jobs>GPU Jobs</h3><p>GPU nodes have multiple GPUs, and very in type (K80 or P100). This means you need to request how many GPUs and of what type that you would like to use.</p><p>To request a gpu of any type, only indicate how many GPUs you would like to use.</p><p>Non-Interactive:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:1 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 SBATCH_SCRIPT.sh
</code></pre></div><p>Interactive</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:4 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 --pty bash -l
</code></pre></div><p>Since the HPCC Cluster has two types of GPUs installed (K80s and P100s), GPUs can be requested explicitly by type.</p><p>Non-Interactive:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:k80:1 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 SBATCH_SCRIPT.sh
sbatch -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:p100:1 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 SBATCH_SCRIPT.sh
</code></pre></div><p>Interactive</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:k80:1 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 --pty bash -l
srun -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:p100:1 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 --pty bash -l
</code></pre></div><p>Of course you should adjust the time argument according to your job requirements.</p><p>Once your job starts your code must reference the environment variable &ldquo;CUDA_VISIBLE_DEVICES&rdquo; which will indicate which GPUs have been assigned to your job. Most CUDA enabled software, like MegaHIT, will check this environment variable and automatically limit accordingly.</p><p>For example, when reserving 4 GPUs for a NAMD2 job:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#204a87>echo</span> <span style=color:#000>$CUDA_VISIBLE_DEVICES</span>
0,1,2,3
namd2 +idlepoll +devices <span style=color:#000>$CUDA_VISIBLE_DEVICES</span> MD1.namd
</code></pre></div><p>Each group is limited to a maximum of 8 GPUs on the gpu partition. Please be respectful of others and keep in mind that the GPU nodes are a limited shared resource.
Since the CUDA libraries will only run with GPU hardward, development and compiling of code must be done within a job session on a GPU node.</p><p>Here are a few more examples of jobs that utilize more complex features (ie. array, dependency, MPI etc):
<a href=https://github.com/ucr-hpcc/hpcc_slurm_examples>Slurm Examples</a></p><h3 id=web-browser-access>Web Browser Access</h3><h4 id=ports>Ports</h4><p>Some jobs require web browser access in order to utilize the software effectively.
These kinds of jobs typically use (bind) ports in order to provide a graphical user interface (GUI) through a web browser.
Users are able to run jobs that use (bind) ports on a compute node.
Any port can be used on any compute node, as long as the port number is greater than 1000 and it is not already in use (bound).</p><h4 id=tunneling>Tunneling</h4><p>Once a job is running on a compute node and bound to a port, you may access this compute node via a web browser.
This is accomplished by using 2 chained SSH tunnels to route traffic through our firewall.
This acts much like 2 runners in a relay race, handing the baton to the next runer, to get past a security checkpoint.</p><p>We will create a tunnel that goes though a headnode and connect to a compute node on a particular port:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh -NL 8888:NodeName:8888 username@cluster.hpcc.ucr.edu
</code></pre></div><p>Port 8888 (first) is the local port you will be using on your laptop.
NodeName is the compute node where where job is running, which can be found by using the <code>squeue -u $USER</code> command.
Port 8888 (second) is the remote port on the compute node.
Again, the NodeName and ports will be different depending on where your job runs and what port your job uses.</p><p>At this point you may need to provide a password to make the SSH tunnel.
Once this has succeeded, the command will hang (this is normal).
Leave this session connected, if you close it your tunnel will be closed.</p><p>Then open a browser on your local computer (PC/laptop) and point it to:</p><pre><code>http://localhost:8888
</code></pre><p>If your job uses TSL/SSL, so you may need to try https if the above does not work:</p><pre><code>https://localhost:8888
</code></pre><h4 id=examples>Examples</h4><p>A perfect example of this method is used for Jupyter Lab/Notebook.
For more details please refer to the following <a href=https://github.com/ucr-hpcc/hpcc_slurm_examples/tree/master/jupyter>Jupyter Example</a>.</p><h3 id=desktop-environments>Desktop Environments</h3><h4 id=vnc-server-cluster>VNC Server (cluster)</h4><p><strong>Start VNC Server</strong></p><p>Log into the cluster:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh username@cluster.hpcc.ucr.edu
</code></pre></div><p>The first time you run the vncserver it will need to be configured:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>vncserver -fg
</code></pre></div><p>You should set a password for yourself, and the read-only password is optional.</p><p>Then configure X Startup with the following command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#39;/usr/bin/ssh-agent /usr/bin/dbus-launch --exit-with-session /usr/bin/gnome-session --session=gnome-classic&#39;</span> &gt; /rhome/<span style=color:#000>$USER</span>/.vnc/xstartup
</code></pre></div><p>After your vncserver is configured, submit a vncserver job to get it started:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch -p short,batch --cpus-per-task<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>4</span> --mem<span style=color:#ce5c00;font-weight:700>=</span>10g --time<span style=color:#ce5c00;font-weight:700>=</span>2:00:00 --wrap<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#39;vncserver -fg&#39;</span> --output<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#39;vncserver-%j.out&#39;</span>
</code></pre></div><blockquote><p>Note: Appropriate job resources should be requested based on the processes you will be running from within the VNC session.</p></blockquote><p>Check the contents of your job log to determine the <code>NodeName</code> and <code>Port</code> you were assigned:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cat vncserver-*.out
</code></pre></div><p>The contents of your slurm job log should be similar to the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>vncserver

New <span style=color:#4e9a06>&#39;i54:1&#39;</span> desktop is i54:1

Creating default startup script /rhome/username/.vnc/xstartup
Starting applications specified in /rhome/username/.vnc/xstartup
Log file is /rhome/username/.vnc/i54:1.log
</code></pre></div><p>The VNC <code>Port</code> used should be 5900+N, N being the display number mentioned above in the format <code>NodeName</code>:<code>DisplayNumber</code> (ie. <code>i54:1</code>).
In this example (default), the port is <code>5901</code>, if this <code>Port</code> were already in use then the vncserver will automatically increment the DisplayNumber and you might find something like <code>i54:2</code> or <code>i54:3</code> and so on.</p><p><strong>Stop VNC Server</strong></p><p>To stop the vncserver, you can click on the logout option from the upper right hand menu from within your VNC desktop environment.
If you want to kill your vncserver manually, then you will need to do the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh NodeName <span style=color:#4e9a06>&#39;vncserver -kill :DisplayNumber&#39;</span>
</code></pre></div><p>You will need to replace <code>NodeName</code> with the node name of your where your job is running, and the <code>DisplayNumber</code> with the DisplayNumber from your slurm job log.</p><h4 id=vnc-client-desktoplaptop>VNC Client (Desktop/Laptop)</h4><p>After you know the <code>NodeName</code> and VNC <code>Port</code> you should be able to create an SSH tunnel to your vncserver, like so:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh -N -L Port:NodeName:Port cluster.hpcc.ucr.edu
</code></pre></div><p>Now let us create an SSH tunnel on your local machine (desktop/laptop) using the <code>NodeName</code> and VNC <code>Port</code> from above:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh -L 5901:i54:5901 cluster.hpcc.ucr.edu
</code></pre></div><p>After you have logged into the cluster with this shell, log into the node where your VNC server is running:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh NodeName
</code></pre></div><p>After you have logged into the correct <code>NodeName</code>, just let this terminal sit here, do not close it.</p><p>Then launch vncviewer on your local system (laptop/workstation), like so:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>vncviewer localhost:5901
</code></pre></div><p>After launching the vncviewer, and providing your VNC password (not your cluster password), you should be able to see a Linux desktop environment.</p><p>For more information regarding tunnels and VNC in MS Windows, please refer <a href=https://docs.ycrc.yale.edu/clusters-at-yale/access/vnc/>More VNC Info</a>.</p><h3 id=licenses>Licenses</h3><p>The cluster currently supports <a href=software_commercial>Commercial Software</a>. Since most of the licenses are campus wide there is no need to track individual jobs. One exception is the Intel Parallel Suite, which contains the Intel compilers.</p><p>The <code>--licenses</code> flag is used to request a license for Intel compilers, for example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun --license<span style=color:#ce5c00;font-weight:700>=</span>intel:1 -p short --mem<span style=color:#ce5c00;font-weight:700>=</span>10g --cpus-per-task<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>10</span> --time<span style=color:#ce5c00;font-weight:700>=</span>2:00:00 --pty bash -l
module load intel
icc -help
</code></pre></div><p>The above interactive submission will request 1 Intel license, 10GB of RAM, 10 CPU cores for 2 hours on the short partition.
The short parititon can only be used for a maximum of 2 hours, however for compilation this could be sufficient.
It is recommended that you separate your compilation job from your computation/analysis job.
This way you will only have the license checked out for the duration of compilation, and not the during the execution of the analysis.</p><h2 id=parallelization>Parallelization</h2><p>There are 3 major ways to parallelize work on the cluster:</p><ol><li>Batch</li><li>Thread</li><li>MPI</li></ol><h3 id=parallel-methods>Parallel Methods</h3><p>For <strong>batch</strong> jobs, all that is required is that you have a way to split up the data and submit multiple jobs running with the different chunks.
Some data sets, for example a FASTA file is very easy to split up (ie. fasta-splitter). This can also be more easily achieved by submitting an array job. For more details please refer to <a href=#advanced-jobs>Advanced Jobs</a>.</p><p>For <strong>threaded</strong> jobs, your software must have an option referring to &ldquo;number of threads&rdquo; or &ldquo;number of processors&rdquo;. Once the thread/processor option is identified in the software, (ie. blastn flag <code>-num_threads 4</code>) you can use that as long as you also request the same number of CPU cores (ie. slurm flag <code>--cpus-per-task=4</code>).</p><p>For <strong>MPI</strong> jobs, your software must be MPI enabled. This generally means that it was compiled with MPI libraries. Please refer to the user manual of the software you wish to use as well as our documentation regarding <a href=#mpi>MPI</a>. It is important that the number of cores used is equal to the number requested.</p><p>In Slurm you will need 2 different flags to request cores, which may seem similar, however they have different purposes:</p><ul><li>The <code>--cpus-per-task=N</code> will provide N number of virtual cores with locality as a factor.
Closer virtual cores can be faster, assuming there is a need for rapid communication between threads.
Generally, this is good for threading, however not so good for independent subprocesses nor for MPI.</li><li>The <code>--ntasks=N</code> flag will provide N number of physical cores on a single or even multiple nodes.
These cores can be further away, since the need for physical CPUs and dedicated memory is more important.
Generally this is good for independent subprocesses, and MPI, however not so good for threading.</li></ul><p>Here is a table to better explain when to use these Slurm options:</p><p>| | Single Threaded | Multi Threaded (OpenMP) | MPI only | MPI + Multi Threaded (hybrid) |</p><table><thead><tr><th>Slurm Flag</th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th></tr></thead><tbody><tr><td><code>--cpus-per-task</code></td><td style=text-align:center></td><td style=text-align:center>X</td><td style=text-align:center></td><td style=text-align:center>X</td></tr><tr><td><code>--ntasks</code></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>X</td><td style=text-align:center>X</td></tr></tbody></table><p>As you can see:</p><ol><li>A single threaded job would use neither Slurm option, since Slurm already assumes at least a single core.</li><li>A multi threaded OpenMP job would use <code>--cpus-per-task</code>.</li><li>A MPI job would use <code>--ntasks</code>.</li><li>A Hybrid job would use both.</li></ol><p>For more details on how these Slurm options work please review <a href=https://slurm.schedmd.com/mc_support.html>Slurm Multi-core/Multi-thread Support</a>.</p><h4 id=mpi>MPI</h4><p>MPI stands for the Message Passing Interface. MPI is a standardized API typically used for parallel and/or distributed computing.
The HPCC cluster has a custom compiled versions of MPI that allows users to run MPI jobs across multiple nodes.
These types of jobs have the ability to take advantage of hundreds of CPU cores symultaniously, thus improving compute time.</p><p>Many implementations of MPI exists, however we only support the following:</p><ul><li><a href=http://www.open-mpi.org/>Open MPI</a></li><li><a href=http://www.mpich.org/>MPICH</a></li><li><a href=https://software.intel.com/en-us/mpi-developer-guide-linux>IMPI</a></li></ul><p>For general information on MPI under Slurm look <a href=https://slurm.schedmd.com/mpi_guide.html>here</a>.
If you need to compile an MPI application then please email <a href=mailto:support@hpcc.ucr.edu>support@hpcc.ucr.edu</a> for assistance.</p><p>When submitting MPI jobs it is best to ensure that the nodes are identical, since MPI is sensitive to differences in CPU and/or memory speeds.
The <code>batch</code> and <code>intel</code> partitions are designed to be homogeneous, however, the <code>short</code> partition is a mixed set of nodes.
When using the <code>short</code> partition for MPI append the constraint flag for Slurm.</p><p><strong>Short Example</strong></p><p>Here is an example that shows how to ensure that your job will only run on <code>intel</code> nodes from the <code>short</code> partition:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch -p short --constraint<span style=color:#ce5c00;font-weight:700>=</span>intel myJobScript.sh
</code></pre></div><p><strong>NAMD Example</strong></p><p>To run a NAMD2 process as an OpenMPI job on the cluster:</p><ol><li><p>Log-in to the cluster</p></li><li><p>Create SBATCH script</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic>#!/bin/bash -l
</span><span style=color:#8f5902;font-style:italic></span>
<span style=color:#8f5902;font-style:italic>#SBATCH -J c3d_cr2_md</span>
<span style=color:#8f5902;font-style:italic>#SBATCH -p batch</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --ntasks=32</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --mem=16gb</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --time=01:00:00</span>

<span style=color:#8f5902;font-style:italic># Load needed modules</span>
<span style=color:#8f5902;font-style:italic># You could also load frequently used modules from within your ~/.bashrc</span>
module load slurm <span style=color:#8f5902;font-style:italic># Should already be loaded</span>
module load openmpi <span style=color:#8f5902;font-style:italic># Should already be loaded</span>
module load namd

<span style=color:#8f5902;font-style:italic># Run job utilizing all requested processors</span>
<span style=color:#8f5902;font-style:italic># Please visit the namd site for usage details: http://www.ks.uiuc.edu/Research/namd/</span>
mpirun --mca btl ^tcp namd2 run.conf <span style=color:#000;font-weight:700>&amp;</span>&gt; run_namd.log
</code></pre></div></li><li><p>Submit SBATCH script to Slurm queuing system</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch run_namd.sh
</code></pre></div></li></ol><p><strong>Maker Example</strong></p><p>OpenMPI does not function properly with Maker, you must use MPICH.
Our version of MPICH does not use the mpirun/mpiexec wrappers, instead use srun:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic>#!/bin/bash -l
</span><span style=color:#8f5902;font-style:italic></span>
<span style=color:#8f5902;font-style:italic>#SBATCH -p intel</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --ntasks=32</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --mem=16gb</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --time=01:00:00</span>

<span style=color:#8f5902;font-style:italic># Load maker</span>
module load maker/2.31.11

mpirun maker <span style=color:#8f5902;font-style:italic># Provide appropriate maker options here</span>

</code></pre></div><h2 id=more-examples>More examples</h2><p>The range of differing jobs and how to submit them is endless:</p><pre><code>1. Singularity containers
2. Database services
3. Graphical user interfaces
4. Etc ...
</code></pre><p>For a growing list of examples please visit <a href=https://github.com/ucr-hpcc/hpcc_slurm_examples>HPCC Slurm Examples</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-87f5b7d632f7c377465149375e5ab359>7 - Package Management</h1><h2 id=python>Python</h2><p>The scope of this manual is a brief introduction on how to manage Python packages.</p><h3 id=python-versions>Python Versions</h3><p>Different Python versions do not play nice with each other. It is best to only load one Python module at any given time.
The miniconda2 module for Python is the default version. This will enable users to leverage the conda installer, but with as few Python packages pre-installed as possible. This is to avoid conflicts with future needs of individuals.</p><h4 id=conda>Conda</h4><p>We have several Conda software modules:</p><ol><li>miniconda2 - Basic Python 2 install (default)</li><li>miniconda3 - Basic Python 3 install</li><li>anaconda2 - Full Python 2 install</li><li>anaconda3 - Full Python 3 install
For more information regarding our module system please refer to <a href=manuals_linux-cluster_start.html#modules>Environment Modules</a>.</li></ol><p>The miniconda modules are very basic installs, however users can choose to unload this basic install for a fuller one (anaconda), like so:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>module load miniconda2 <span style=color:#8f5902;font-style:italic>#This is the default</span>
</code></pre></div><p>After loading anaconda, you will see that there are many more Python packages installed (ie. numpy, scipy, pandas, jupyter, etc&mldr;).
For a list of installed Python packages try the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>pip list
</code></pre></div><h4 id=virtual-environments>Virtual Environments</h4><p>Sometimes it is best to create your own environment in which you have full control over package installs.
Conda allows you to do this through virtual environments.</p><h5 id=initialize>Initialize</h5><p>Conda will now auto initialize when you load the corresponding module. No need to run the <code>conda init</code> or make any modifications to your <code>~/.bashrc</code> file.</p><h5 id=configure>Configure</h5><p>Installing many packages can consume a large (ie. >20GB) amount of disk space, thus it is recommended to store conda environments under your bigdata space.
If you have bigdata, create the <code>.condarc</code> file (otherwise conda environments will be created under your home directory).</p><p>Create the file <code>.condarc</code> in your home, with the following content:</p><pre><code>channels:
  - defaults
pkgs_dirs:
  - ~/bigdata/.conda/pkgs
envs_dirs:
  - ~/bigdata/.conda/envs
auto_activate_base: false
</code></pre><p>Then create your Python 2 conda environment, like so:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>conda create -n NameForNewEnv <span style=color:#000>python</span><span style=color:#ce5c00;font-weight:700>=</span>2.7.14 <span style=color:#8f5902;font-style:italic># Many Python versions are available</span>
</code></pre></div><p>For Python 3, please use the miniconda3, like so:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>module unload miniconda2
module load miniconda3
conda create -n NameForNewEnv <span style=color:#000>python</span><span style=color:#ce5c00;font-weight:700>=</span>3.6.4 <span style=color:#8f5902;font-style:italic># Many Python versions are available</span>
</code></pre></div><h5 id=activating>Activating</h5><p>Once your virtual environment has been created, you need to activate it before you can use it:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>conda activate NameForNewEnv
</code></pre></div><h5 id=deactivating>Deactivating</h5><p>In order to exit from your virtual environment, do the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>conda deactivate
</code></pre></div><h5 id=installing-packages>Installing packages</h5><p>Here is a simple example for installing packages under your Python virtual environment via conda:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>conda install -n NameForNewEnv PackageName
</code></pre></div><p>You may need to enable an additional channel to install the package (refer to your package&rsquo;s documentation):</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>conda install -n NameForNewEnv -c ChannelName PackageName
</code></pre></div><h5 id=cloning>Cloning</h5><p>It is possible for you to copy an existing environment into a new environment:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>conda create --name AnotherNameForNewEnv --clone NameForNewEnv
</code></pre></div><h5 id=listing-environments>Listing Environments</h5><p>Run the following to get a list of currently installed conda evironments:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>conda env list
</code></pre></div><h5 id=removing>Removing</h5><p>If you wish to remove a conda environment run the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>conda env remove --name myenv
</code></pre></div><h4 id=more-info>More Info</h4><p>For more information regarding conda please visit <a href=https://conda.io/docs/user-guide/>Conda Docs</a>.</p><h3 id=jupyter>Jupyter</h3><p>You can run jupyter as an interactive job using <a href=manuals_linux-cluster_jobs.html#web-browser-access>tunneling</a>, or you can use the web portal <a href=https://jupyter.hpcc.ucr.edu>Jupyter-Hub</a>.</p><h4 id=virtual-environment>Virtual Environment</h4><p>In order to enable your conda virtual environemnt within the Jupyter web portal you will need to do the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic># Create a virtual environment, if you don&#39;t already have one</span>
conda create -n ipykernel_py2 <span style=color:#000>python</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>2</span> ipykernel

<span style=color:#8f5902;font-style:italic># Load the new environment</span>
conda activate ipykernel_py2

<span style=color:#8f5902;font-style:italic># Install kernel</span>
python -m ipykernel install --user --name myenv --display-name <span style=color:#4e9a06>&#34;JupyterPy2&#34;</span>
</code></pre></div><p>Now when you visit <a href=https://jupyter.hpcc.ucr.edu>Jupyter-Hub</a> you should see the option &ldquo;JupyterPy2&rdquo; when you click the &ldquo;New&rdquo; dropdown menu in the upper left corner of the home page.</p><h4 id=r>R</h4><p>For instructions on how to configure your R environment please visit <a href=https://github.com/IRkernel/IRkernel>IRkernel</a>.
Since we should already have IRkernel install in the latest version of R, you would only need to do the following within R:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#000>IRkernel</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>installspec</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>name</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#4e9a06>&#39;ir44&#39;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>displayname</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#4e9a06>&#39;R 4.0.1&#39;</span><span style=color:#000;font-weight:700>)</span>
</code></pre></div><h2 id=r-1>R</h2><p>This section is regarding how to manage R packages.</p><h3 id=current-r-version>Current R Version</h3><p>Currently the default version of R is 4.0.1 and is loaded automatically for you.
This can be seen by running:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>module list
</code></pre></div><h3 id=older-r-versions>Older R Versions</h3><p>You can load older versions of R with the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>module unload R
module load R/3.4.2
</code></pre></div><h3 id=installing-r-packages>Installing R Packages</h3><p>The default version of R has many of the most popular R packages available already installed.
It is also possible for you to install additional R packages in your local environments.</p><h4 id=bioconductor-packages>Bioconductor Packages</h4><p>To install from Bioconductor you can use the following method:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#000>BiocManager</span><span style=color:#ce5c00;font-weight:700>::</span><span style=color:#000>install</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>c</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;package-to-install&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#4e9a06>&#34;another-packages-to-install&#34;</span><span style=color:#000;font-weight:700>))</span>
<span style=color:#000>Update</span> <span style=color:#000>all</span><span style=color:#ce5c00;font-weight:700>/</span><span style=color:#000>some</span><span style=color:#ce5c00;font-weight:700>/</span><span style=color:#000>none</span><span style=color:#ce5c00;font-weight:700>?</span> <span style=color:#000>[a</span><span style=color:#ce5c00;font-weight:700>/</span><span style=color:#000>s</span><span style=color:#ce5c00;font-weight:700>/</span><span style=color:#000>n]</span><span style=color:#ce5c00;font-weight:700>:</span> <span style=color:#000>n</span>
</code></pre></div><p>For more information please visit <a href=https://www.bioconductor.org/install/>Bioconductor Install Page</a>.</p><h4 id=github-packages>GitHub Packages</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#000>library</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>devtools</span><span style=color:#000;font-weight:700>)</span>
<span style=color:#000>install_github</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;duncantl/RGoogleDocs&#34;</span><span style=color:#000;font-weight:700>)</span> <span style=color:#8f5902;font-style:italic># replace name with the GitHub account/repo</span>
</code></pre></div><h4 id=local-packages>Local Packages</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-R data-lang=R><span style=color:#000>install.packages</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;http://hartleys.github.io/QoRTs/QoRTs_LATEST.tar.gz&#34;</span><span style=color:#000;font-weight:700>,</span><span style=color:#000>repos</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>NULL</span><span style=color:#000;font-weight:700>,</span><span style=color:#000>type</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;source&#34;</span><span style=color:#000;font-weight:700>)</span> <span style=color:#8f5902;font-style:italic># replace URL with your URL or local path to your .tar.gz file</span>
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2da4a1c0ba6afe09bb67a3b123aee2f9>8 - Parallel Evaluations in R</h1><h1 id=overview>Overview</h1><p>R provides a variety of packages for parallel computations. One of the most
comprehensive parallel computing environments for R is <a href=https://mllg.github.io/batchtools/articles/batchtools.html><code>batchtools</code></a>
(formerly <code>BatchJobs</code>). It supports both multi-core and multi-node computations with and
without schedulers. By making use of cluster template files, most schedulers
and queueing systems are also supported (e.g. Torque, Sun Grid Engine, Slurm).</p><h2 id=r-code-of-this-section>R code of this section</h2><p>To simplify the evaluation of the R code of this page, the corresponding text version
is available for download from <a href=https://raw.githubusercontent.com/ucr-hpcc/ucr-hpcc.github.io/master/_support_docs/tutorials/batchtools_test.R>here</a>.</p><h2 id=parallelization-with-batchtools>Parallelization with batchtools</h2><p>The following introduces the usage of <code>batchtools</code> for a computer cluster using SLURM as scheduler (workload manager).</p><h2 id=set-up-working-directory-for-slurm>Set up working directory for SLURM</h2><p>First login to your cluster account, open R and execute the following lines. This will
create a test directory (here <code>mytestdir</code>), redirect R into this directory and then download
the required files:</p><ul><li><a href=https://github.com/ucr-hpcc/ucr-hpcc.github.io/blob/master/_support_docs/tutorials/slurm.tmpl><code>slurm.tmpl</code></a></li><li><a href=https://github.com/ucr-hpcc/ucr-hpcc.github.io/blob/master/_support_docs/tutorials/.batchtools.conf.R><code>.batchtools.conf.R</code></a></li></ul><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=color:#000>dir.create</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;mytestdir&#34;</span><span style=color:#000;font-weight:700>)</span>
<span style=color:#000>setwd</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;mytestdir&#34;</span><span style=color:#000;font-weight:700>)</span>
<span style=color:#000>download.file</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;https://goo.gl/tLMddb&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#4e9a06>&#34;slurm.tmpl&#34;</span><span style=color:#000;font-weight:700>)</span>
<span style=color:#000>download.file</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;https://goo.gl/5HrYkE&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#4e9a06>&#34;.batchtools.conf.R&#34;</span><span style=color:#000;font-weight:700>)</span>
</code></pre></div><h2 id=load-package-and-define-some-custom-function>Load package and define some custom function</h2><p>This is the test function (here toy example) that will be run on the cluster for demonstration
purposes. It subsets the <code>iris</code> data frame by rows, and appends the host name and R version of each
node where the function was executed. The R version to be used on each node can be
specified in the <code>slurm.tmpl</code> file (under <code>module load</code>).</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=color:#000>library</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#39;RenvModule&#39;</span><span style=color:#000;font-weight:700>)</span>
<span style=color:#000>module</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#39;load&#39;</span><span style=color:#000;font-weight:700>,</span><span style=color:#4e9a06>&#39;slurm&#39;</span><span style=color:#000;font-weight:700>)</span> <span style=color:#8f5902;font-style:italic># Loads slurm among other modules</span>

<span style=color:#000>library</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>batchtools</span><span style=color:#000;font-weight:700>)</span>
<span style=color:#000>myFct</span> <span style=color:#ce5c00;font-weight:700>&lt;-</span> <span style=color:#000>function</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>x</span><span style=color:#000;font-weight:700>)</span> <span style=color:#000;font-weight:700>{</span>
	<span style=color:#000>result</span> <span style=color:#ce5c00;font-weight:700>&lt;-</span> <span style=color:#000>cbind</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>iris[x</span><span style=color:#000;font-weight:700>,</span> <span style=color:#0000cf;font-weight:700>1</span><span style=color:#ce5c00;font-weight:700>:</span><span style=color:#0000cf;font-weight:700>4</span><span style=color:#000;font-weight:700>,</span><span style=color:#000>]</span><span style=color:#000;font-weight:700>,</span>
	<span style=color:#000>Node</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>system</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;hostname&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>intern</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>TRUE</span><span style=color:#000;font-weight:700>),</span>
	<span style=color:#000>Rversion</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>paste</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>R.Version</span><span style=color:#000;font-weight:700>()</span><span style=color:#000>[6</span><span style=color:#ce5c00;font-weight:700>:</span><span style=color:#0000cf;font-weight:700>7</span><span style=color:#000>]</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>collapse</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;.&#34;</span><span style=color:#000;font-weight:700>))</span>
<span style=color:#000;font-weight:700>}</span>
</code></pre></div><h2 id=submit-jobs-from-r-to-cluster>Submit jobs from R to cluster</h2><p>The following creates a <code>batchtools</code> registry, defines the number of jobs and resource requests, and then submits the jobs to the cluster
via SLURM.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=color:#000>reg</span> <span style=color:#ce5c00;font-weight:700>&lt;-</span> <span style=color:#000>makeRegistry</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>file.dir</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;myregdir&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>conf.file</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;.batchtools.conf.R&#34;</span><span style=color:#000;font-weight:700>)</span>
<span style=color:#000>Njobs</span> <span style=color:#ce5c00;font-weight:700>&lt;-</span> <span style=color:#0000cf;font-weight:700>1</span><span style=color:#ce5c00;font-weight:700>:</span><span style=color:#0000cf;font-weight:700>4</span> <span style=color:#8f5902;font-style:italic># Define number of jobs (here 4)</span>
<span style=color:#000>ids</span> <span style=color:#ce5c00;font-weight:700>&lt;-</span> <span style=color:#000>batchMap</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>fun</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>myFct</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>x</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>Njobs</span><span style=color:#000;font-weight:700>)</span> 
<span style=color:#000>done</span> <span style=color:#ce5c00;font-weight:700>&lt;-</span> <span style=color:#000>submitJobs</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>ids</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>reg</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>reg</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>resources</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>list</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>partition</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;short&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>walltime</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>60</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>ntasks</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>ncpus</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>memory</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>1024</span><span style=color:#000;font-weight:700>))</span>
<span style=color:#000>waitForJobs</span><span style=color:#000;font-weight:700>()</span> <span style=color:#8f5902;font-style:italic># Wait until jobs are completed</span>
</code></pre></div><h2 id=summarize-job-status>Summarize job status</h2><p>After the jobs are completed one instect their status as follows.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=color:#000>getStatus</span><span style=color:#000;font-weight:700>()</span> <span style=color:#8f5902;font-style:italic># Summarize job status</span>
<span style=color:#000>showLog</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>Njobs[1]</span><span style=color:#000;font-weight:700>)</span>
<span style=color:#8f5902;font-style:italic># killJobs(Njobs) # # Possible from within R or outside with scancel</span>
</code></pre></div><h2 id=accessassemble-results>Access/assemble results</h2><p>The results are stored as <code>.rds</code> files in the registry directory (here <code>myregdir</code>). One
can access them manually via <code>readRDS</code> or use various convenience utilities provided
by the <code>batchtools</code> package.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=color:#000>readRDS</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;myregdir/results/1.rds&#34;</span><span style=color:#000;font-weight:700>)</span> <span style=color:#8f5902;font-style:italic># reads from rds file first result chunk</span>
<span style=color:#000>loadResult</span><span style=color:#000;font-weight:700>(</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>)</span> 
<span style=color:#000>lapply</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>Njobs</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>loadResult</span><span style=color:#000;font-weight:700>)</span>
<span style=color:#000>reduceResults</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>rbind</span><span style=color:#000;font-weight:700>)</span> <span style=color:#8f5902;font-style:italic># Assemble result chunks in single data.frame</span>
<span style=color:#000>do.call</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;rbind&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>lapply</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>Njobs</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>loadResult</span><span style=color:#000;font-weight:700>))</span>
</code></pre></div><h2 id=remove-registry-directory-from-file-system>Remove registry directory from file system</h2><p>By default existing registries will not be overwritten. If required one can exlicitly
clean and delete them with the following functions.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=color:#000>clearRegistry</span><span style=color:#000;font-weight:700>()</span> <span style=color:#8f5902;font-style:italic># Clear registry in R session</span>
<span style=color:#000>removeRegistry</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>wait</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>reg</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>reg</span><span style=color:#000;font-weight:700>)</span> <span style=color:#8f5902;font-style:italic># Delete registry directory</span>
<span style=color:#8f5902;font-style:italic># unlink(&#34;myregdir&#34;, recursive=TRUE) # Same as previous line</span>
</code></pre></div><h2 id=load-registry-into-r>Load registry into R</h2><p>Loading a registry can be useful when accessing the results at a later state or
after moving them to a local system.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=color:#000>from_file</span> <span style=color:#ce5c00;font-weight:700>&lt;-</span> <span style=color:#000>loadRegistry</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>&#34;myregdir&#34;</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>conf.file</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;.batchtools.conf.R&#34;</span><span style=color:#000;font-weight:700>)</span>
<span style=color:#000>reduceResults</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>rbind</span><span style=color:#000;font-weight:700>)</span>
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-dcbb75e455948c3241cb60aaa145d06c>9 - Queue Policies</h1><h2 id=start-times>Start Times</h2><p>Start times are a great way to track your jobs:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>squeue -u <span style=color:#000>$USER</span> --start
</code></pre></div><p>Start times are rough estimates based on the current state of the queue.</p><h2 id=fair-share>Fair-Share</h2><p>Users that have not submitted any jobs in a long time usually have a higher priority over others that have ran jobs recently.
Thus the estimated start times can be extended to allow everyone their fair share of the system.
This prevents a few large groups from dominating the queuing system for long periods of time.</p><p>You can see with the <code>sqmore</code> command what priority your job has (list is sorted from lowest to highest priority).
You can also check to see how your group&rsquo;s priority is compared to other groups on the cluster with the &ldquo;sshare&rdquo; command.</p><p>For example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sshare
</code></pre></div><p>It may also be useful to see your entire group&rsquo;s fairshare score and who has used the most shares:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sshare -A <span style=color:#000>$GROUP</span> --all
</code></pre></div><p>Lastley, if you only want to see your own fairshare score:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sshare -A <span style=color:#000>$GROUP</span> -u <span style=color:#000>$USER</span>
</code></pre></div><p>The fairshare score is a number between 0 and 1. The best score being 1, and the worst being 0.
The fairshare score approches zero the more resource you (or your group) consume.
Your individual consumption of resources (usage) does affect your entire group&rsquo;s fiarshare score.
The affects of your running/completed jobs on your fairshare score are halved each day (half-life).
Thus, after waiting several days without running any jobs, you should see an improvment in your fairshare score.</p><p>Here is a very good <a href=https://www.rc.fas.harvard.edu/fairshare/>explaination of fairshare</a>.</p><h2 id=priority>Priority</h2><p>The fairshare score and jobs queue wait time is used to calculate your job&rsquo;s priority.
You can use the <code>sprio</code> command to check the priority of your jobs:</p><pre><code>sprio -u $USER
</code></pre><p>Even if your group has a lower fairshare score, your job may still have a very high priority.
This would be likely due to the job&rsquo;s queue wait time, and it should start as soon as possible regardless of fairshare score.
You can use the <code>sqmore</code> command to see a list of all jobs sorted by priority.</p><h2 id=backfill>Backfill</h2><p>Some small jobs may start before yours, only if they can complete before yours starts and thus not negatively affecting your start time.</p><h2 id=priority-partition>Priority Partition</h2><p>Some groups on our system have purchased additional hardware. These nodes will not be affected by the fairshare score.
This is because jobs submitted to the group&rsquo;s partition will be evaluated first before any other jobs that have been submitted to those nodes from a different partition.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5f2edcb7e51abae3e3abc6d5929e6ded>10 - Security</h1><h2 id=protection-levels-and-classification>Protection Levels and Classification</h2><p>UCR protection levels, and data classifications are outlined by UCOP as a UC wide policy: <a href=https://security.ucop.edu/policies/institutional-information-and-it-resource-classification.html>UCOP Institutional Information and IT Resource Classification</a>
According to the above documentation, there are 4 levels of protection for 4 classifications of data:</p><p>| Protection Level | Policy | Examples |
|&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&ndash;|
| P1 - Minimal | IS-1 | Internet facing websites, press releases, anything intended for public use |
| P2 - Low | IS-2 | Unpublished research work, intellectual property NOT classified as P3 or P4 |
| P3 - Moderate | IS-3 | Research information classified by an Institutional Review Board as P3 (ie. dbGaP from NIH) |
| P4 - High | IS-4 | Protected Health Information (PHI/HIPAA), patient records, sensitive identifiable human subject research data, Social Security Numbers |</p><p>The HPC cluster could be compliant with with other security polices (ie. NIH), however the policy must be reviewed by our security team.</p><p>At this time the HPC cluster is not a IS-4 (P4) compliant cluster. If you have needs for very sensitive data, it may be best to work with UCSD and their <a href=https://sherlock.sdsc.edu/>Sherlock service</a>.
Our cluster is IS-3 compliant, however there are several responsibilities that users will need to adhere to.</p><h2 id=general-guidelines>General Guidelines</h2><p><span style=color:red>First, please contact us (<a href=mailto:support@hpcc.ucr.edu>support@hpcc.ucr.edu</a>) before transferring any data to the cluster.
After we have reviewed your needs, data classification and appropriate protection level, then it may be possible to proceed to use the HPCC.</span></p><p>Here are a few basic rules to keep in mind:</p><ul><li>Always be aware of access control methods (Unix permissions and ACLs), do not allow others to view the data (ie. chmod 400 filename)</li><li>Do not make unnecessary copies of the data</li><li>Do not transfer the data to insecure locations</li><li>Encrypt data when/where possible</li><li>Delete all data when it is no longer needed</li></ul><h2 id=access-controls>Access Controls</h2><p>When sharing files with others, it is imperative that proper permission are used.
However, basic Unix permissions (user,group,other) may not be adequate.
It is better to use ACLs in order to allow fine grained access to sensitive files.</p><h3 id=gpfs-acls>GPFS ACLs</h3><p>GPFS is used for most of our filesystems (/rhome and /bigdata) and it uses nfsv4 style ACLs.
Users are able to explicitly allow many individuals, or groups, access to specific files or directories.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic># Get current permissions and store in acls file</span>
mmgetacl /path/to/file &gt; ~/acls.txt

<span style=color:#8f5902;font-style:italic># Edit acls file containing permissions</span>
vim ~/acls.txt

<span style=color:#8f5902;font-style:italic># Apply new permissions to file</span>
mmputacl -i ~/acls.txt /path/to/file

<span style=color:#8f5902;font-style:italic># Delete acls file</span>
rm ~/acls.txt
</code></pre></div><p>For more information regarding GPFS ACLs refer to the following: <a href=https://www.ibm.com/support/knowledgecenter/en/STXKQY_4.2.3/com.ibm.spectrum.scale.v4r23.doc/bl1adm_nfsv4syn.htm>GPFS ACLs</a></p><h3 id=xfs-acls>XFS ACLs</h3><p>The XFS filesystem is used for the CentOS operating system and typical unix locations (/,/var,/tmp,etc), as well as /secure.
For more information on how to use ACLs under XFS, please refer to the following: <a href=https://vishmule.com/2015/06/11/access-control-list-acl-permissions-in-rhel7centos7/>CentOS 7 XFS</a></p><blockquote><p>Note: ACLs are not applicable to gocryptfs, which is a FUSE filesystem, not GPFS nor XFS.</p></blockquote><h2 id=encryption>Encryption</h2><p>Under the IS-3 policy, P3 data encryption is mandatory.
It is best if you get into the habit of doing encryption in transit, as well as encryption at rest.
This means, when you move the data (transit) or when the data is not in use (rest), it should be encrypted.</p><h3 id=in-transit>In Transit</h3><p>When transferring files make sure that files are encrypted in flight with one of the following transfer protocols:</p><ul><li>SCP</li><li>SFTP</li><li>RSYNC (via SSH)</li></ul><p>The destination for sensitive data on the cluster must also be encrypted at rest under one of the follow secure locations:</p><ul><li>/dev/shm/ - This location is in RAM, so it does not exist at rest (ensure proper ACLs)</li><li>/secure - This location is encrypted at rest with AES 256 key length (ensure proper ACLs)</li><li>/run/user/$EUID/unencrypted - This location is manually managed, and should be created for access to unencrypted files.</li></ul><p>It is also possible to encrypt your files with GPG (<a href=https://kb.iu.edu/d/awio>GPG Example</a>), before they are transferred.
Thus, during transfer they will be GPG encrypted. However, decryption must occur in one of the secure locations mentioned above.</p><blockquote><p>Note: Never store passphrases/passwords/masterkeys in an unsecure location (ie. a plain text script under /rhome).</p></blockquote><h3 id=at-rest>At Rest</h3><p>There are 3 methods available on the cluster for encryption at rest:</p><ol><li>GPG encryption of files via the command line <a href=https://kb.iu.edu/d/awio>GPG Example</a>, however you must ensure proper ACLs and decryption must occur in a secure location.</li><li>The location &ldquo;/secure&rdquo; is encrypted and is mounted on the head nodes, however you must ensure proper ACLs.</li><li>Create your own location with <a href=https://nuetzlich.net/gocryptfs/forward_mode_crypto/>gocryptfs</a>.</li></ol><h4 id=gocryptfsmgr>GocryptfsMgr</h4><p>You can use <code>gocryptfs</code> directly or use the <code>gocryptfsmgr</code>, which automates a few steps in order to simplify things.</p><p>Here are the basics when using <code>gocryptfsmgr</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic># Create new encrypted data directory</span>
gocryptfsmgr create bigdata privatedata1

<span style=color:#8f5902;font-style:italic># List all encrypted and unencrypted (access point) directories</span>
gocryptfsmgr list

<span style=color:#8f5902;font-style:italic># Unencrypted privatedata1 (create access point)</span>
gocryptfsmgr open bigdata privatedata1 rw

<span style=color:#8f5902;font-style:italic># Transfer files (ie. SCP,SFTP,RSYNC)</span>
scp user@remote-server:sensitive_file.txt <span style=color:#000>$UNENCRYPTED</span>/privatedata1/sensitive_file.txt

<span style=color:#8f5902;font-style:italic># Remove access point (re-encrypt) privatedata1</span>
gocryptfsmgr close privatedata1

<span style=color:#8f5902;font-style:italic># Remove all access points (re-encrypt all)</span>
gocryptfsmgr quit
</code></pre></div><p>For subsequent access to the encrypted space, (ie. computation or analysis) the follow procedure is recommended:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic># Request a 2hr interactive job on an exclusive node, resources can be adjusted as needed</span>
srun -p short --exclusive<span style=color:#ce5c00;font-weight:700>=</span>user --pty bash -l

<span style=color:#8f5902;font-style:italic># Unencrypted privatedata1 in read-only mode (create access point)</span>
gocryptfsmgr open bigdata privatedata1 ro

<span style=color:#8f5902;font-style:italic># Read file contents from privatedata1 (simulating work or analysis)</span>
cat <span style=color:#000>$UNENCRYPTED</span>/privatedata1/sensitive_file.txt

<span style=color:#8f5902;font-style:italic># List all encrypted and unencrypted (access points) directories</span>
gocryptfsmgr list

<span style=color:#8f5902;font-style:italic># Make sure we re-encrypt (close access point) for privatedata1</span>
gocryptfsmgr close privatedata1

<span style=color:#8f5902;font-style:italic># Exit from interactive job</span>
<span style=color:#204a87>exit</span>
</code></pre></div><p>With the above methods you can create multiple encrypted directories and access points and move between them.</p><h4 id=gocryptfs>Gocryptfs</h4><p>When using the <code>gocryptfs</code> directly, you will need to know a bit more details on how it works.
The <code>gocryptfs</code> module on the HPCC cluster uses these predefined variables:</p><ol><li><code>HOME_ENCRYPTED</code> = <code>/rhome/$USER/encrypted</code> - Very small encrypted space, not recommended to use</li><li><code>BIGDATA_ENCRYPTED</code> = <code>/rhome/$USER/bigdata/encrypted</code> - Best encrypted space for private data sets</li><li><code>SHARED_ENCRYPTED</code> = <code>/rhome/$USER/shared/encrypted</code> - Encrypted space when intending to share data sets with group</li><li><code>UNENCRYPTED</code> = <code>/run/user/$UID/unencrypted</code> - Access directory where encrypted data will be viewed as unencrypted</li></ol><p>Here is an example how to create an encrypted directory under the <code>BIGDATA_ENCRYPTED</code> location using <code>gocryptfs</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic># Load gocyptfs software</span>
module load gocryptfs

<span style=color:#8f5902;font-style:italic># Create empty data directory</span>
mkdir -p <span style=color:#000>$BIGDATA_ENCRYPTED</span>/privatedata1

<span style=color:#8f5902;font-style:italic># Then intialize empty directory and encrypt it</span>
gocryptfs -aessiv -init <span style=color:#000>$BIGDATA_ENCRYPTED</span>/privatedata1

<span style=color:#8f5902;font-style:italic># Create access point directory where encrypted files will be viewed as unencrypted</span>
mkdir -p <span style=color:#000>$UNENCRYPTED</span>/privatedata1

<span style=color:#8f5902;font-style:italic># After that mount the encrypted directory on the access point and open a new shell within it</span>
gocryptfssh <span style=color:#000>$BIGDATA_ENCRYPTED</span>/privatedata1

<span style=color:#8f5902;font-style:italic># Transfer files (ie. SCP,SFTP,RSYNC)</span>
scp user@remote-server:sensitive_file.txt <span style=color:#000>$UNENCRYPTED</span>/sensitive_file.txt

<span style=color:#8f5902;font-style:italic># Exiting this shell will automatically unmount the unencrypted directory</span>
<span style=color:#204a87>exit</span>
</code></pre></div><p>For subsequent access to the encrypted space, (ie. computation or analysis) the follow procedure is recommended:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic># Request a 2hr interactive job on an exclusive node, resources can be adjusted as needed</span>
srun -p short --exclusive<span style=color:#ce5c00;font-weight:700>=</span>user --pty bash -l

<span style=color:#8f5902;font-style:italic># Load cyptfs software</span>
module load gocryptfs

<span style=color:#8f5902;font-style:italic># Create unencrypted directory</span>
mkdir -p <span style=color:#000>$UNENCRYPTED</span>/privatedata1

<span style=color:#8f5902;font-style:italic># Mount encrypted filesystem as read-only and unmount idling for 1 hour</span>
gocryptfs -ro -i 1h -sharedstorage <span style=color:#000>$BIGDATA_ENCRYPTED</span>/privatedata1 <span style=color:#000>$UNENCRYPTED</span>/privatedata1

<span style=color:#8f5902;font-style:italic># Read file contents (simulating work or analysis)</span>
cat <span style=color:#000>$UNENCRYPTED</span>/privatedata1/sensitive_file.txt

<span style=color:#8f5902;font-style:italic># Manually close access point when analysis has completed</span>
fusermount -u <span style=color:#000>$UNENCRYPTED</span>/privatedata1

<span style=color:#8f5902;font-style:italic># Delete old empty access point</span>
rmdir <span style=color:#000>$UNENCRYPTED</span>/privatedata1
</code></pre></div><blockquote><p>WARNING: Avoid writing to the same file at the same time from different nodes. The encrypted file system cannot handle simultaneous writes and will corrupt the file. If simultaneous jobs are necessary then using write mode from a head node and read-only mode from compute nodes may be the best solution here.
Also, be mindful of reamaining job time and make sure that you have unmounted the unencrypted directories before your job ends.</p></blockquote><p>For another example on how to use gocrypfs on an HPC cluster: <a href=https://hpc.uni.lu/blog/2018/sensitive-data-encryption-using-gocryptfs/>Luxembourg HPC gocryptfs Example</a></p><h2 id=deletion>Deletion</h2><p>To ensure the complete removal of data, it is best to <code>shred</code> files instead of removing them with <code>rm</code>. The <code>shred</code> program will overwrite the contents of a file with randomized data such that recovery of this file will be very difficult, if not impossible.</p><p>Instead of using the common <code>rm</code> command to delete something, please use the <code>shred</code> command, like so:</p><pre><code>shred -u somefile
</code></pre><p>The above command will overwrite the file with random data, and then remove (unlink) it.</p><p>If we want to be even more secure, we can pass over the file seven times to ensure that reconstruction is nearly impossible, then remove it:</p><pre><code>shred -v -n 6 -z -u somefile
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-a3377e68a8e7a29ab44c288bf6043b45>11 - Sharing Data</h1><h2 id=permissions>Permissions</h2><p>It is useful to share data and results with other users on the cluster, and we encourage collaboration The easiest way to share a file is to place it in a location that both users can access. Then the second user can simply copy it to a location of their choice. However, this requires that the file permissions permit the second user to read the file.
Basic file permissions on Linux and other Unix like systems are composed of three groups: owner, group, and other. Each one of these represents the permissions for different groups of people: the user who owns the file, all the group members of the group owner, and everyone else, respectively Each group has 3 permissions: read, write, and execute, represented as r,w, and x. For example the following file is owned by the user <code>username</code> (with read, write, and execute), owned by the group <code>groupname</code> (with read and execute), and everyone else cannot access it.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>username@pigeon:~$ ls -l myFile
-rwxr-x---   <span style=color:#0000cf;font-weight:700>1</span> username groupname 1.6K Nov <span style=color:#0000cf;font-weight:700>19</span> 12:32 myFile
</code></pre></div><p>If you wanted to share this file with someone outside the <code>groupname</code> group, read permissions must be added to the file for &lsquo;other&rsquo;:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>username@pigeon:~$ chmod o+r myFile
</code></pre></div><p>To learn more about ownership, permissions, and groups please visit <a href=manuals_linux-basics_permissions>Linux Basics Permissions</a>.</p><h2 id=set-default-permissions>Set Default Permissions</h2><p>In Linux, it is possible to set the default file permission for new files. This is useful if you are collaborating on a project, or frequently share files and you do not want to be constantly adjusting permissions The command responsible for this is called &lsquo;umask&rsquo;. You should first check what your default permissions currently are by running &lsquo;umask -S&rsquo;.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>username@pigeon:~$ <span style=color:#204a87>umask</span> -S
<span style=color:#000>u</span><span style=color:#ce5c00;font-weight:700>=</span>rwx,g<span style=color:#ce5c00;font-weight:700>=</span>rx,o<span style=color:#ce5c00;font-weight:700>=</span>rx
</code></pre></div><p>To set your default permissions, simply run umask with the correct options. Please note, that this does not change permissions on any existing files, only new files created after you update the default permissions. For instance, if you wanted to set your default permissions to you having full control, your group being able to read and execute your files, and no one else to have access, you would run:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>username@pigeon:~$ <span style=color:#204a87>umask</span> <span style=color:#000>u</span><span style=color:#ce5c00;font-weight:700>=</span>rwx,g<span style=color:#ce5c00;font-weight:700>=</span>rx,o<span style=color:#ce5c00;font-weight:700>=</span>
</code></pre></div><p>It is also important to note that these settings only affect your current session.
If you log out and log back in, these settings will be reset.
To make your changes permanent you need to add them to your <code>.bashrc</code> file, which is a hidden file in your home directory (if you do not have a <code>.bashrc</code> file, you will need to create an empty file called <code>.bashrc</code> in your home directory).
Adding umask to your <code>.bashrc</code> file is as simple as adding your umask command (such as <code>umask u=rwx,g=rx,o=r</code>) to the end of the file.
Then simply log out and back in for the changes to take affect. You can double check that the settings have taken affect by running <code>umask -S</code>.</p><p>To learn more about umask please visit <a href=http://www.cyberciti.biz/tips/understanding-linux-unix-umask-value-usage.html>What is Umask and How To Setup Default umask Under Linux?</a>.</p><h2 id=copying-bigdata>Copying bigdata</h2><p>Rsync can:</p><ul><li>Copy (transfer) directories between different locations</li><li>Perform transfers over the network via SSH</li><li>Compare large data sets (<code>-n, --dry-run</code> option)</li><li>Resume interrupted transfers</li></ul><p>Rsync Notes:</p><ul><li>Rsync can be used on Windows, but you must install <a href=https://cygwin.com>Cygwin</a>. Most Mac and Linux systems already have rsync install by default.</li><li>Always put the / after both folder names, e.g: <code>FOLDER_A/</code> Failing to do so will result in the nesting folders every time you try to resume. If you don&rsquo;t put / you will get a second folder_B inside folder_B <code>FOLDER_A/FOLDER_A/</code></li><li>Rsync only copies by default.</li><li>Once the rsync command is done, run it again. The second run will be shorter and can be used as a double check. If there was no output from the second run then nothing changed.</li><li>To learn more try <code>man rsync</code></li></ul><p>If you are transfering to, or from, your laptop/workstation it is required that you run the rsync command locally from your laptop/workstation.</p><p>To transfer to the cluster:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>rsync -av --progress FOLDER_A/ cluster.hpcc.ucr.edu:FOLDER_A/
</code></pre></div><p>To transfer from the cluster:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>rsync -av --progress cluster.hpcc.ucr.edu:FOLDER_A/ FOLDER_A/
</code></pre></div><p>Rsync will use SSH and will ask you for your cluster password, the same way SSH or SCP does.</p><p>If your rsync transer was interrupted, rsync can continue where it left off. Simply run the same command again to resume.</p><h2 id=copying-large-folders-on-the-cluster-between-directories>Copying large folders on the cluster between Directories</h2><p>If you want to syncronize the contents from one directory to another, then use the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>rsync -av --progress PATH_A/FOLDER_A/ PATH_B/FOLDER_A/
</code></pre></div><p>Rsync does not move but only copies. Thus you would need to delete the original once you confirm that everything has been transfered.</p><h2 id=copying-large-folders-between-the-cluster-and-other-servers>Copying large folders between the cluster and other servers</h2><p>If you want to copy data from the cluster to your own server, or another remote system, use the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>rsync -ai FOLDER_A/ sever2.xyz.edu:FOLDER_A/
</code></pre></div><p>The sever2.xyz.edu machine must be a server that accepts Rsync connections via SSH.</p><h2 id=sharing-files-on-the-web>Sharing Files on the Web</h2><p>Simply create a symbolic link or move the files into your html directory when you want to share them.
For exmaple, log into the HPC cluster and run the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic># Make sure you have an html directory</span>
mkdir ~/.html

<span style=color:#8f5902;font-style:italic>#Make sure permissions are set correctly</span>
chmod a+x ~/
chmod a+rx ~/.html

<span style=color:#8f5902;font-style:italic># Make a new web project directory</span>
mkdir www-project
chmod a+rx www-project

<span style=color:#8f5902;font-style:italic># Create a default test file</span>
<span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#39;&lt;h1&gt;Hello!&lt;/h1&gt;&#39;</span> &gt; ~/www-project/index.html

<span style=color:#8f5902;font-style:italic># Create shortcut/link for new web project in html directory </span>
ln -s ~/www-project ~/.html/
</code></pre></div><p>Now, test it out by pointing your web-browser to <a href=https://cluster.hpcc.ucr.edu/~username/www-project/>https://cluster.hpcc.ucr.edu/~username/www-project/</a>
Be sure to replace <code>username</code> with your actual user name.</p><h2 id=password-protect-web-pages>Password Protect Web Pages</h2><p>Files in web directories can be password protected.
First create a password file and then create a new user:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>touch ~/.html/.htpasswd
htpasswd ~/.html/.htpasswd newwebuser
</code></pre></div><p>This will prompt you to enter a password for the new user &lsquo;newwebuser&rsquo;.
Create a new directory, or go to an existing directory, that you want to password protect:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>mkdir ~/.html/locked_dir
<span style=color:#204a87>cd</span> ~/.html/locked_dir
</code></pre></div><p>For the above commands you can choose any directory name you want.</p><p>Then place the following content within a file called <code>.htaccess</code>:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-apache data-lang=apache><span style=color:#204a87>AuthName</span> &#39;Please login&#39;
<span style=color:#204a87>AuthType</span> Basic
<span style=color:#204a87>AuthUserFile</span> <span style=color:#4e9a06>/rhome/username/.html/.htpasswd</span>
<span style=color:#204a87>require</span> <span style=color:#204a87;font-weight:700>user</span> newwebuser
</code></pre></div><p>Now, test it out by pointing your web-browser to <a href=http://cluster.hpcc.ucr.edu/~username/locked_dir>http://cluster.hpcc.ucr.edu/~username/locked_dir</a>
Be sure to replace <code>username</code> with your actual user name for the above code and URL.</p><h2 id=google-drive>Google Drive</h2><p>There are several tools used to transfer files from Google Drive to the cluster, however RClone may be the easiest to setup.</p><ol><li><p>Create an <code>SSH</code> tunnel to the cluster, (MS Windows users should use <code>MobaXterm</code>):</p><pre><code>ssh -L 53682:localhost:53682 username@cluster.hpcc.ucr.edu
</code></pre></li><li><p>Once you have logged into the cluster with the above command, then load <code>rclone</code> via the module system and run it, like so:</p><pre><code>module load rclone
rclone config
</code></pre></li><li><p>After that, follow this <a href=https://rclone.org/drive/>RClone Walkthrough</a> to complete your setup.</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-96d8c142fd8b2269c2f97cff55ef365f>12 - SSH Keys Apple macOS</h1><h2 id=ssh-keys-on-macos>SSH Keys on macOS</h2><h3 id=what-are-ssh-keys>What are SSH Keys?</h3><p>SSH (Secure Shell) keys are an access credential that is used in the SSH protocol.</p><p>The private key remains on the system being used to access the HPCC cluster and is used to decrypt information that is exchanged in the transfer between the HPCC cluster and your system.</p><p>A public key file is used to encrypt information, and is stored on your own system.
The public key file is stored on the HPCC cluster and contains a list of authorized public keys.</p><h3 id=why-do-you-need-ssh-keys>Why do you need SSH Keys?</h3><p>HPCC supports two authentication methods; <code>Password+DUO</code> and <code>SSH Keys</code>.
The <code>Password+DUO</code> method requires a UCR NetID, if you do not have this then you will need to use <code>SSH keys</code> in order to access the HPCC cluster.</p><h3 id=what-you-need>What you need</h3><h4 id=filezilla>Filezilla</h4><p>You will need to install <code>Filezilla</code> in order to transfer the public SSH key to the HPCC cluster.</p><ol><li>Download the <code>Filezilla Client</code> for Mac OS X <a href=https://filezilla-project.org>here</a>.<ul><li>Make sure your Mac OS X system is updated to the latest version.</li></ul></li><li>Follow the install wizard to complete the install of <code>Filezilla</code>.</li></ol><h4 id=sourcetree>Sourcetree</h4><p>You will need to install <code>Sourcetree</code> in order to generate your <code>SSH keys</code> (or use the command line method mentioned here: <a href=some_other_page>Manage SSH Keys via Command Line</a>.</p><ol><li>Download <code>Sourcetree</code> from <a href=https://www.sourcetreeapp.com>here</a>.</li><li>Click on <code>Download for Mac OS X</code>.</li><li>Install <code>Sourcetree</code>.</li></ol><h3 id=create-ssh-keys-sourcetree>Create SSH Keys (<code>Sourcetree</code>)</h3><ol><li><p>Open the <code>Sourcetree</code> application and under the top <code>Sourcetree</code> menu click on the <code>Preferences...</code> sub-menu item.</p><p><img src=/img/41.png alt=fig0></p></li><li><p>Navigate to <code>Accounts</code> category and click on <code>Add...</code>.</p><p><img src=/img/42.png alt=fig0></p></li><li><p>Click on <code>Auth Type:</code> and change the drop down menu from <code>OAuth</code> to <code>Basic</code>. Make sure <code>Protocol:</code> is set to <code>SSH</code> in the drop down menu.</p><p><img src=/img/43.png alt=fig0></p></li><li><p>Enter <code>id_rsa</code> in the <code>Username</code> field.</p><p><img src=/img/44.png alt=fig0></p></li><li><p>Click the <code>Generate Key</code> button.</p><p><img src=/img/50.png alt=fig1></p></li><li><p>Press <code>Cancel</code> to exit out of the window.</p></li></ol><h3 id=ssh-keys-location>SSH Keys Location</h3><p>By default, your key files are created in the path: <code>/Users/macOSUsername/.ssh/</code>.</p><p>To verify that the keys were created, do the following:</p><ol><li><p>Open a new finder window. Click on your home directory on the left side pane.</p><p><img src=/img/23.png alt=fig1></p></li><li><p>Press the 3-button combo <code>Command</code>+<code>Shift</code>+<code>.</code> together (visualized below) to see hidden folders:</p><p><img src=/img/47b.png alt=fig1></p></li><li><p>You will now be able to see your <code>.ssh</code> folder, open it by double-clicking.</p><p><img src=/img/48.png alt=fig1></p></li><li><p>You should see your newly generated pair of <code>SSH key</code> files in the folder.</p><p><img src=/img/51.png alt=fig1></p></li><li><p>Sourcetree adds the <code>-Bitbucket</code> to the end of the <code>SSH key</code> file names. Remove this by clicking on the file you want to rename and press the <code>Enter</code> key which allows us to rename the file before the extension.</p><p><img src=/img/52.png alt=fig1></p></li><li><p>After you have removed the <code>-Bitbucket</code> suffix from each of the <code>SSH key</code> file names, your new <code>SSH key</code> file names should be <code>id_rsa</code> and <code>id_rsa.pub</code>.</p><p><img src=/img/53.png alt=fig1></p></li></ol><h3 id=configure-ssh-keys>Configure SSH Keys</h3><h4 id=public-ssh-key>Public SSH Key</h4><p>Now that you have created your <code>SSH keys</code>, and renamed them, you will need to placed the public key (<code>id_rsa.pub</code>) on the cluster using the <code>cluster.hpcc.ucr.edu</code> server.</p><ol><li><p>Start the <code>Filezilla</code> application.</p></li><li><p>Fill in the <code>Quickconnect</code> fields at the top of the application window:</p><ul><li>Enter your HPCC username in the <code>Username</code> field.</li><li>Enter the HPCC servername <code>cluster.hpcc.ucr.edu</code> for the <code>Host</code> field.</li><li>Enter your password in the <code>Password</code> field.</li><li>Enter <code>22</code> in the <code>Port</code> field.</li></ul><p><img src=/img/1e.png alt=fig4></p></li><li><p>Click on <code>Quickconnect</code></p><p><img src=/img/8e.png alt=fig7></p></li><li><p>If a pop up prompts you to save your password, select the <code>Save passwords</code> option, then click the <code>OK</code> button.</p></li><li><p>If the next pop up prompts you, then check the box that states <code>Always trust this host, add this key to the cache</code>, then click the <code>OK</code> button.</p><p><img src=/img/6be.png alt=fig8></p></li><li><p>Now that you are connected to Filezilla transfer your public SSH key from your macOS system by dragging the file <code>/Users/macOSUsername/.ssh/id_rsa.pub</code> and dropping it into the HPCC cluster direcotry <code>/rhome/username/.ssh/</code>.</p><p><img src=/img/4e.png alt=fig10></p></li></ol><h4 id=private-ssh-key>Private SSH Key</h4><p>Once your public key is in place, now you can configure <code>Filezilla</code> to use your private <code>SSH key</code> and connect to the cluster through the <code>secure.hpcc.ucr.edu</code> server.</p><ol><li><p>Open Filezilla <code>Site Manager</code> button in the top bar of icons.</p><p><img src=/img/60.png alt=fig3></p></li><li><p>Click on <code>New Site</code>, rename it (optional) and press enter.</p><p><img src=/img/54.png alt=fig3></p></li><li><p>Make sure the following fields are correctly filled before adding your <code>SSH key</code> file:</p><ul><li><code>Protocol</code>: should be set to <code>SFTP - SSH File Transfer Protocol</code></li><li><code>Host</code>: type in <code>secure.hpcc.ucr.edu</code></li><li><code>Port</code>: type <code>22</code></li><li><code>Logon Type</code>: set to <code>Key file</code></li><li><code>User</code>: type in your HPCC username</li></ul><p>After these fields are finalized, click the <code>Browse..</code> button.</p><p><img src=/img/56.png alt=fig4></p></li><li><p>Navigate to the folder you saved your key file in (default location is <code>/Users/macOSUsername/.ssh</code>) and open the private key file <code>id_rsa</code>.</p><p><img src=/img/57.png alt=fig4></p></li><li><p>You should see the added keyfile in the <code>Key file:</code> box, then click <code>Connect</code>.</p><p><img src=/img/59.png alt=fig5></p><p>Subsequnt connections can be done from the <code>Quickconnect</code> history by clicking on the down arrow to the right side of the <code>Quickconnect</code> button.</p><p><img src=/img/61.png alt=fig5></p></li><li><p>Remember to select the <code>secure.hpcc.ucr.edu</code> address.</p><p><img src=/img/62.png alt=fig5></p></li><li><p>Transfer files by double clicking or drag-n-drop. For more details regarding file transfers vist <a href=some_other_page>Filezilla Usage</a>.</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-a7ae8375cc7ee8eeaa194b633a0ec206>13 - SSH Keys Microsoft Windows</h1><h2 id=ssh-keys-on-ms-windows>SSH Keys on MS Windows</h2><h3 id=what-are-ssh-keys>What are SSH keys?</h3><p>SSH (Secure Shell) keys are an access credential that is used in the SSH protocol.</p><p>The private key remains on the system being used to access the HPCC cluster and is used to decrypt information that is exchanged in the transfer between the HPCC cluster and your system.</p><p>A public key file is used to encrypt information, and is stored on your own system.
The public key file is stored on the HPCC cluster and contains a list of authorized public keys.</p><h3 id=why-do-you-need-ssh-keys>Why do you need SSH keys?</h3><p>HPCC supports two authentication methods; <code>Password+DUO</code> and <code>SSH Keys</code>.
The <code>Password+DUO</code> method requires a UCR NetID, if you do not have this then you will need to use <code>SSH keys</code> in order to access the HPCC cluster.</p><h3 id=what-you-need>What you need</h3><h4 id=filezilla>Filezilla</h4><p>You will need to install <code>Filezilla</code> in order to transfer the public SSH key to the HPCC cluster.</p><ol><li>Download the <code>Filezilla Client</code> for Windows <a href="https://filezilla-project.org/download.php?show_all=1">here</a>.
* Make sure your Windows system is updated to the latest version.</li><li>Follow the install wizard to complete the install of <code>Filezilla</code>.</li></ol><h4 id=mobaxterm>MobaXterm</h4><p>You will need to install <code>MobaXterm</code> in order to generate your <code>SSH keys</code> and also to transfer the keys to the cluster.</p><ol><li>Download <code>MobaXterm</code> from <a href=https://mobaxterm.mobatek.net/download-home-edition.html><code>here</code></a>.</li><li>Unzip</li><li>Double click portable version of exe and run the <code>MobaXterm</code> application.</li></ol><h3 id=create-ssh-keys-mobaxterm>Create SSH Keys (<code>MobaXterm</code>)</h3><ol><li><p>Begin by clicking on the tools drop down on the upper menu bar</p><p><img src=/img/ssh1moba.png alt=mobasshkey1></p></li><li><p>Find and click on the MobaKeyGen (SSH key generator) option</p><p><img src=/img/ssh2moba.png alt=mobasshkey2></p></li><li><p>A window should appear to create a new SSH key. Click on generate to create a new SSH key pair. Follow the on menu instructions.</p><p><img src=/img/revisedkeygen.png alt=revisedkeygen></p></li><li><p>Once your key has been created, enter a password in the key passphrase field to password protect your key. Click on <code>conversions</code> in the tool bar and click on <code>Export OpenSSH Key</code>. Save this key as <code>id_rsa</code> and put the file in an easy to access location.
Click on <code>Save private key</code> to save the private key with an extension of <code>.ppk</code> to use with MobaXterm or FileZilla. Save the key as <code>mobaxterm_privkey</code> and put the file in an easy to access location.</p><p><img src=/img/revisedkeygen2.png alt=revisedkeygen2></p></li><li><p>Highlight EVERYTHING in the box labeled &ldquo;Public key for pasting into OpenSSH authorized_keys file&rdquo; then right-click on it and choose Copy. Open <code>Notepad</code> and paste the copied text. Save the file as <code>id_rsa.pub</code> and put the file in an easy to access location.</p><p><img src=/img/revisedkeygen3.png alt=revisedkeygen3></p></li></ol><h3 id=keys-location>Keys Location</h3><p>SSH keys should be saved under the location <code>C:\Users\username\.ssh</code>.</p><p><img src=/img/sshkeyloc.png alt=sshkeyloc></p><h3 id=configure-ssh-keys>Configure SSH Keys</h3><h4 id=public-ssh-key>Public SSH Key</h4><p>Now that you have created your <code>SSH keys</code>, and renamed them, you will need to placed the public key (<code>id_rsa.pub</code>) on the cluster using the <code>cluster.hpcc.ucr.edu</code></p><ol><li><p>Start the <code>Filezilla</code> application.</p></li><li><p>Fill in the <code>Quickconnect</code> fields at the top of the application window:</p><ul><li>Enter your HPCC username in the <code>Username</code> field.</li><li>Enter the HPCC servername <code>cluster.hpcc.ucr.edu</code> for the <code>Host</code> field.</li><li>Enter your password in the <code>Password</code> field.</li><li>Enter <code>22</code> in the <code>Port</code> field.</li></ul><p><img src=/img/filezilla1.png alt=filezilla1></p></li><li><p>Click on <code>Quickconnect</code></p><p><img src=/img/filezilla2.png alt=filezilla2></p></li><li><p>If the next pop up prompts you, then check the box that states <code>Always trust this host, add this key to the cache</code>, then click the <code>OK</code> button.</p><p><img src=/img/filezilla3.png alt=filezilla3></p></li><li><p>You will need to create a <code>.ssh</code> directory to hold your SSH keys. On the right hand side, right click and click on the <code>Create directory option</code> under your home folder location.
<img src=/img/createsshdir.png alt=createsshdir></p></li><li><p>A window will appear to name the new directory. Name should be the following format: <code>/rhome/username/.ssh</code>. After naming the new directory click on <code>OK</code>.
<img src=/img/createsshdir2.png alt=createsshdir></p></li><li><p>Right click on the new <code>.ssh</code> directory that has been created. Find and click on <code>File permissions</code>.
<img src=/img/createsshdir3.png alt=createsshdir></p></li><li><p>A window with the directory permissions will appear. The <code>.ssh</code> directory needs exact permissions in order for it to function properly. Follow the image below to apply the permissions.
<img src=/img/createsshdir4.png alt=createsshdir></p></li><li><p>Now that you are connected to Filezilla transfer your public SSH key from your system by dragging the file <code>id_rsa.pub</code> and dropping it into the HPCC cluster direcotry <code>/rhome/username/.ssh/</code>.</p><p><img src=/img/filezilla4.png alt=filezilla4></p></li></ol><h4 id=private-ssh-key>Private SSH Key</h4><p>Once your public key is in place, now you can configure <code>Filezilla</code> to use your private <code>SSH key</code> and connect to the cluster through the <code>secure.hpcc.ucr.edu</code> server.</p><ol><li><p>Open Filezilla <code>Site Manager</code> button in the top bar of icons.</p><p><img src=/img/filezilla5.png alt=filezilla5></p></li><li><p>Click on <code>New Site</code>, rename it (optional) and press enter.</p><p><img src=/img/filezilla6.png alt=filezilla6></p></li><li><p>Make sure the following fields are correctly filled before adding your <code>SSH key</code> file:</p><ul><li><code>Protocol</code>: should be set to <code>SFTP - SSH File Transfer Protocol</code></li><li><code>Host</code>: type in <code>secure.hpcc.ucr.edu</code></li><li><code>Port</code>: type <code>22</code></li><li><code>Logon Type</code>: set to <code>Key file</code></li><li><code>User</code>: type in your HPCC username</li></ul><p>After these fields are finalized, click the <code>Browse..</code> button.</p><p><img src=/img/filezilla7.png alt=filezilla7></p></li><li><p>Navigate to the folder you saved your private key file in and open the private key file <code>mobaxterm_privkey.ppk</code>. You should see the added keyfile in the <code>Key file:</code> box, then click <code>Connect</code>.</p><p><img src=/img/filezilla9.png alt=filezilla9></p></li><li><p>Subsequnt connections can be done from the <code>Quickconnect</code> history by clicking on the down arrow to the right side of the <code>Quickconnect</code> button. Remember to select the <code>secure.hpcc.ucr.edu</code> address.</p><p><img src=/img/filezilla11.png alt=filezilla11></p></li><li><p>Transfer files by double clicking or drag-n-drop. For more details regarding file transfers vist <a href=some_other_page>Filezilla Usage</a>.</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-6ac662661d338690e787bb2d160108ef>14 - Terminal-based Working Environments</h1><h1 id=terminal-ides>Terminal IDEs</h1><p>This page introduces several terminal-based working environments available on UCR&rsquo;s
HPC cluster that are useful for a variety of computer languages.</p><h2 id=vimnvim-basics>Vim/Nvim Basics</h2><p>To work efficiently on remote systems like a computer cluster, it is essential
to learn how to work in a pure command-line interface. GUI environments like
RStudio and similar coding environments are not suitable for this. In addition,
there is a lot of value of knowing how to work in an environment that is not
restricted to a specific programming language. Therefore, for working on remote
systems like HPCC Cluster, this site focuses on Nvim and Tmux. Both are useful
for many programming languages. Combinded with the <code>nvim-r</code> plugin they also
provide a powerful command-line working environment for R. Users of Emacs may
want to consider using <a href=https://ess.r-project.org/>ESS</a> instead. The following
provides a brief introduction to the Nvim-R-Tmux environment.</p><h3 id=vim-overview>Vim overview</h3><p>The following opens a file (here <code>myfile</code>) with nvim (or vim)</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>nvim myfile.txt <span style=color:#8f5902;font-style:italic># for neovim (or &#39;vim myfile.txt&#39; for vim)</span>
</code></pre></div><p>Once you are in Nvim, there are three main modes: normal, insert and command mode. The most important commands for switching between the three modes are:</p><ul><li><code>i</code>: The <code>i</code> key brings you from the normal mode to the insert mode. The latter is used for typing.</li><li><code>Esc</code>: The <code>Esc</code> key brings you from the insert mode back to the normal mode.</li><li><code>:</code>: The <code>:</code> key starts the command mode at the bottom of the screen.</li></ul><p>Use the arrow keys to move your cursor in the text. Using <code>Fn Up/Down key</code> allows to page through
the text quicker. In the following command overview, all commands starting with <code>:</code> need to be typed in the command mode.
All other commands are typed in the normal mode after pushing the <code>Esc</code> key.</p><p>Important modifier keys to control vim/nvim</p><ul><li><code>:w</code>: save changes to file. If you are in editing mode you have to hit <code>Esc</code> first.</li><li><code>:q</code>: quit file that has not been changed</li><li><code>:wq</code>: save and quit file</li><li><code>:!q</code>: quit file without saving any changes</li></ul><h3 id=useful-resources-for-learning-vimnvim>Useful resources for learning vim/nvim</h3><ul><li><a href=http://www.openvim.com>Interactive Vim Tutorial</a></li><li><a href=http://vimdoc.sourceforge.net/>Official Vim Documentation</a></li><li><a href=http://hpcc.ucr.edu/manuals_linux-basics_vim.html>HPCC Linux Manual</a></li></ul><h2 id=for-r-nvim-r>For R: nvim-R</h2><h3 id=basics>Basics</h3><p>Tmux is a terminal multiplexer that allows to split terminal windows and to detach/reattach to
existing terminal sessions. Combinded with the <code>nvim-r</code> plugin it provides a powerful command-line working
environment for R where users can send code from a script to the R console or command-line.
Both tmux and the <code>nvim-r</code> plugin need to be installed on a system. On HPCC Cluster both are configured
in each user account. If this is not the case then follow the quick configuration instructions given in the following subsection.</p><center><img title=Nvim-R src=https://raw.githubusercontent.com/jalvesaq/Nvim-R/master/Nvim-R.gif></center>
<center>Nvim-R IDE for R</center><h3 id=quick-configuration-in-user-accounts>Quick configuration in user accounts</h3><p>Skip these steps if Nvim-R-Tmux is already configured in your account. Or follow the <a href=https://gist.github.com/tgirke/7a7c197b443243937f68c422e5471899>detailed
instructions</a> to install Nvim-R-Tmux from scratch on your own system.</p><ol><li>Log in to your user account on HPCC and execute <code>install_nvimRtmux</code>. Alternatively, follow these step-by-step <a href=https://cluster.hpcc.ucr.edu/~tgirke/Documents/R_BioCond/My_R_Scripts/vim-r-plugin/README_nvimRtmux>install commands</a>.</li><li>To enable the nvim-R-tmux environment, log out and in again.</li><li>Follow usage instructions of next section.</li></ol><h3 id=basic-usage-of-nvim-r-tmux>Basic usage of Nvim-R-Tmux</h3><p>The official and much more detailed user manual for <code>Nvim-R</code> is available <a href=https://github.com/jalvesaq/Nvim-R/blob/master/doc/Nvim-R.txt>here</a>.
The following gives a short introduction into the basic usage of Nvim-R-Tmux:</p><p><strong>1. Start tmux session</strong> (optional)</p><p>Note, running Nvim from within a tmux session is optional. Skip this step if tmux functionality is not required (<em>e.g.</em> reattaching to sessions on remote systems).</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>tmux <span style=color:#8f5902;font-style:italic># starts a new tmux session </span>
tmux a <span style=color:#8f5902;font-style:italic># attaches to an existing session </span>
</code></pre></div><p><strong>2. Open nvim-connected R session</strong></p><p>Open a <code>*.R</code> or <code>*.Rmd</code> file with <code>nvim</code> and intialize a connected R session with <code>\rf</code>. This command can be remapped to other key combinations, e.g. uncommenting lines 10-12 in <code>.config/nvim/init.vim</code> will remap it to the <code>F2</code> key. Note, the resulting split window among Nvim and R behaves like a split viewport in <code>nvim</code> or <code>vim</code> meaning the usage of <code>Ctrl-w w</code> followed by <code>i</code> and <code>Esc</code> is important for navigation.</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>nvim myscript.R <span style=color:#8f5902;font-style:italic># or *.Rmd file</span>
</code></pre></div><p><strong>3. Send R code from nvim to the R pane</strong></p><p>Single lines of code can be sent from nvim to the R console by pressing the space bar. To send
several lines at once, one can select them in nvim&rsquo;s visual mode and then hit the space bar.
Please note, the default command for sending code lines in the nvim-r-plugin is <code>\l</code>. This key
binding has been remapped in the provided <code>.config/nvim/init.vim</code> file to the space bar. Most other key bindings (shortcuts) still start with the <code>\</code> as LocalLeader, <em>e.g.</em> <code>\rh</code> opens the help for a function/object where the curser is located in nvim. More details on this are given below.</p><h3 id=important-keybindings-for-nvim>Important keybindings for nvim</h3><p>The main advantages of Neovim compared to Vim are its better performance and its built-in terminal emulator facilitating the communication among Neovim and interactive programming environments such as R. Since the Vim and Neovim environments are managed independently, one can run them in parallel on the same system without interfering with each other. The usage of Neovim is almost identical to Vim.</p><p><strong>Nvim commands</strong></p><ul><li><code>\rf</code>: opens vim-connected R session. If you do this the first time in your user account, you might be asked to create an <code>R</code> directory under <code>~/</code>. If so approve this action by pressing <code>y</code>.</li><li><code>spacebar</code>: sends code from vim to R; here remapped in <code>init.vim</code> from default <code>\l</code></li><li><code>:split</code> or <code>:vsplit</code>: splits viewport (similar to pane split in tmux)</li><li><code>gz</code>: maximizes size of viewport in normal mode (similar to Tmux&rsquo;s <code>Ctrl-a z</code> zoom utility)</li><li><code>Ctrl-w w</code>: jumps cursor to R viewport and back; toggle between insert (<code>i</code>) and command (<code>Esc</code>) mode is required for navigation and controlling the environment.</li><li><code>Ctrl-w r</code>: swaps viewports</li><li><code>Ctrl-w =</code>: resizes splits to equal size</li><li><code>:resize &lt;+5 or -5></code>: resizes height by specified value</li><li><code>:vertical resize &lt;+5 or -5></code>: resizes width by specified value</li><li><code>Ctrl-w H</code> or <code>Ctrl-w K</code>: toggles between horizontal/vertical splits</li><li><code>Ctrl-spacebar</code>: omni completion for R objects/functions when nvim is in insert mode. Note, this has been remapped in <code>init.vim</code> from difficult to type default <code>Ctrl-x Ctrl-o</code>.</li><li><code>:h nvim-R</code>: opens nvim-R&rsquo;s user manual; navigation works the same as for any Vim/Nvim help document</li><li><code>:Rhelp fct_name</code>: opens help for a function from nvim&rsquo;s command mode with text completion support</li><li><code>Ctrl-s and Ctrl-x</code>: freezes/unfreezes vim (some systems)</li></ul><h3 id=important-keybindings-for-tmux>Important keybindings for tmux</h3><p><strong>Pane-level commands</strong></p><ul><li><code>Ctrl-a %</code>: splits pane vertically</li><li><code>Ctrl-a "</code>: splits pane horizontally</li><li><code>Ctrl-a o</code>: jumps cursor to next pane</li><li><code>Ctrl-a Ctrl-o</code>: swaps panes</li><li><code>Ctrl-a &lt;space bar></code>: rotates pane arrangement</li><li><code>Ctrl-a Alt &lt;left or right></code>: resizes to left or right</li><li><code>Ctrl-a Esc &lt;up or down></code>: resizes to left or right</li></ul><p><strong>Window-level comands</strong></p><ul><li><code>Ctrl-a n</code>: switches to next tmux window</li><li><code>Ctrl-a Ctrl-a</code>: switches to previous tmux window</li><li><code>Ctrl-a c</code>: creates a new tmux window</li><li><code>Ctrl-a 1</code>: switches to specific tmux window selected by number</li></ul><p><strong>Session-level comands</strong></p><ul><li><code>Ctrl-a d</code>: detaches from current session</li><li><code>Ctrl-a s</code>: switch between available tmux sesssions</li><li><code>$ tmux new -s &lt;name></code>: starts new session with a specific name</li><li><code>$ tmux ls</code>: lists available tmux session(s)</li><li><code>$ tmux attach -t &lt;id></code>: attaches to specific tmux session</li><li><code>$ tmux attach</code>: reattaches to session</li><li><code>$ tmux kill-session -t &lt;id></code>: kills a specific tmux session</li><li><code>Ctrl-a : kill-session</code>: kills a session from tmux command mode that can be initiated with <code>Ctrl-a :</code></li></ul><h2 id=for-bash-python-and-other-languages>For Bash, Python and other languages</h2><h3 id=basics-1>Basics</h3><p>For languages other than R one can use the
<a href=https://github.com/jalvesaq/vimcmdline>vimcmdline</a> plugin for nvim (or vim).
Supported languages include Bash, Python, Golang, Haskell, JavaScript, Julia,
Jupyter, Lisp, Macaulay2, Matlab, Prolog, Ruby, and Sage. The nvim terminal
also colorizes the output, as in the screenshot below, where different colors
are used for general output, positive and negative numbers, and the prompt
line.</p><center><img title=vimcmdline src=https://cloud.githubusercontent.com/assets/891655/7090493/5fba2426-df71-11e4-8eb8-f17668d9361a.png></center>
<center>vimcmdline</center><h3 id=install>Install</h3><p>To install it, one needs to copy from the <code>vimcmdline</code> resository the directories
<code>ftplugin</code>, <code>plugin</code> and <code>syntax</code> and their files to <code>~/.config/nvim/</code>. For
user accounts of UCR’s HPCC, the above install script <code>install_nvimRtmux</code> includes the
install of <code>vimcmdline</code> (since 09-Jun-18).</p><h3 id=usage>Usage</h3><p>The usage of <code>vimcmdline</code> is very similar to <code>nvim-R</code>. To start a connected terminal session, one
opens with nvim a code file with the extension of a given language (<em>e.g.</em> <code>*.sh</code> for Bash or <code>*.py</code> for Python),
while the corresponding interactive interpreter session is initiated
by pressing the key sequence <code>\s</code> (corresponds to <code>\rf</code> under <code>nvim-R</code>). Subsequently, code lines can be sent
with the space bar. More details are available <a href=https://github.com/jalvesaq/vimcmdline>here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-25afe2f58c1a6825ea3c6ea9a5dc3fe3>15 - Visualization</h1><h2 id=gpu-workstation>GPU Workstation</h2><p>The High-Performance Computing Center at UCR has a GPU workstation specifically designed for rendering high resolution 3D graphics.</p><h3 id=hardware>Hardware</h3><ul><li>Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz</li><li>DDR4 256GB @ 2400 MHz</li><li>NVIDIA Corporation GM204GL [Quadro M5000]</li><li>1TB RAID 1 HDD</li></ul><h3 id=software>Software</h3><p>The GPU workstation is uniquely configured to be an extension of the HPCC cluster. Thus, all software available to the cluster is also available on the GPU workstation through <a href=manuals_linux-cluster_start.html#modules>Environment Modules</a>.</p><h3 id=access>Access</h3><p>The GPU workstation is currently located in the Genomics building room 1208. Please check ahead of time to make sure the machine is available <a href=mailto:support@hpcc.ucr.edu>support@hpcc.ucr.edu</a>.
Once you have access to the GPU workstation, login with your cluster credentials. If your username does not appear in the list, you may need to click <code>Not listed?</code> at the bottom of the screen so that you are able to type in your username.</p><h4 id=usage>Usage</h4><p>There are 2 ways to use the GPU workstation:</p><ol><li>Local - Run processes directly on the GPU workstation hardware</li><li>Remote - Run processes remotely on the GPU cluster hardware</li></ol><p><strong>Local</strong></p><p>Local usage is very simple. Open a terminal and use the <a href=manuals_linux-cluster_start.html#modules>Environment Modules</a> to load the desired software, then run your software from the terminal.
For example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>module load amira
Amira
</code></pre></div><p><strong>Remotely</strong></p><p>Open a terminal and submit a job. This is to reserve the time on the remote GPU node. Then once your job has started connect to the remote GPU node via ssh and forward the graphics back to the GPU workstation.
For example:</p><ol><li><p>Submit a job for March 28th, 2018 at 9:30am for a duration of 24 hours, 4 cpus, 100GB memory:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch --begin<span style=color:#ce5c00;font-weight:700>=</span>2018-03-28T09:30:00 --time<span style=color:#ce5c00;font-weight:700>=</span>24:00:00 -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:1 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --cpus-per-task<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>4</span> --wrap<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#39;echo ${CUDA_VISIBLE_DEVICES} &gt; ~/.CUDA_VISIBLE_DEVICES; sleep infinity&#39;</span>
</code></pre></div><p>Read about <a href=manuals_linux-cluster_jobs.html#gpu-jobs>GPU jobs</a> for more information regarding the above.</p></li><li><p>Run the VirtualGL client in order to receive 3D graphics from the remove GPU node:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>vglclient <span style=color:#000;font-weight:700>&amp;</span>
</code></pre></div></li><li><p>Wait for the job to start, and then check where your job is running:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#000>GPU_NODE</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>$(</span>squeue -h -p gpu -u <span style=color:#000>$USER</span> -o <span style=color:#4e9a06>&#39;%N&#39;</span><span style=color:#204a87;font-weight:700>)</span><span style=color:#000;font-weight:700>;</span> <span style=color:#204a87>echo</span> <span style=color:#000>$GPU_NODE</span>
</code></pre></div></li><li><p>The above command should result in a GPU node name, which you then need to SSH directly into with the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh -XY <span style=color:#000>$GPU_NODE</span>
</code></pre></div></li><li><p>Once you have SSH&rsquo;ed into the remote GPU node, run setup the environment and run your software:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#204a87>export</span> <span style=color:#000>NO_AT_BRIDGE</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>1</span>
module load amira
vglrun -display :<span style=color:#204a87;font-weight:700>$(</span>head -1 ~/.CUDA_VISIBLE_DEVICES<span style=color:#204a87;font-weight:700>)</span> Amira
</code></pre></div></li></ol></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank rel="noopener noreferrer" href=https://example.org/mail><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank rel="noopener noreferrer" href=https://example.org/twitter><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank rel="noopener noreferrer" href=https://example.org/stack><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank rel="noopener noreferrer" href=https://github.com/google/docsy><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank rel="noopener noreferrer" href=https://example.org/slack><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Developer mailing list" aria-label="Developer mailing list"><a class=text-white target=_blank rel="noopener noreferrer" href=https://example.org/mail><i class="fa fa-envelope"></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2021 The Docsy Authors All Rights Reserved</small>
<small class=ml-1><a href=https://policies.google.com/privacy target=_blank>Privacy Policy</a></small><p class=mt-2><a href=/about/>Abouts</a></p></div></div></div></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js integrity=sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy crossorigin=anonymous></script><script src=/js/main.min.5c74b870c6953931a705f390a49c7e4c0a842ec5c83b24354758dd674343ed0d.js integrity="sha256-XHS4cMaVOTGnBfOQpJx+TAqELsXIOyQ1R1jdZ0ND7Q0=" crossorigin=anonymous></script></body></html>