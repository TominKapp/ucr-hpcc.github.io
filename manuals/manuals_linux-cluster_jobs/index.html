<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.83.1"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Linux Cluster - Managing Jobs | HPCC Home</title><meta property="og:title" content="Linux Cluster - Managing Jobs"><meta property="og:description" content="What is a Job? Submitting and managing jobs is at the heart of using the cluster. A &lsquo;job&rsquo; refers to the script, pipeline or experiment that you run on the nodes in the cluster.
Partitions In the past we used queues under the old Torque system, we now refer to these logically grouped nodes as partitions. There are several different partitions available for cluster users to send jobs to:
 intel  Default partition Nodes: i01-02,i17-i40 Cores: Intel, 256 per user RAM: 1 GB default Time (walltime): 168 hours (7 days) default   batch  Nodes: c01-c48 Cores: AMD, 256 per user RAM: 1 GB default Time (walltime): 168 hours (7 days) default   highmem  Nodes: h01-h06 Cores: Intel, 32 per user RAM: 100 GB min and 1000 GB max Time (walltime): 48 hours (2 days) default   gpu  Nodes: gpu01-gpu05 GPUs: 8 per group RAM: 1 GB default Time (walltime): 48 hours (2 days) default   short  Nodes: Mixed set of nodes from batch, intel, and group partitions Cores: AMD/Intel, 256 per user RAM: 1 GB default Time (walltime): 2 hours Maximum   Group Partition  This partition is unique to the group, if your lab has purchased nodes then you will have a priority partition with the same name as your group (ie."><meta property="og:type" content="article"><meta property="og:url" content="/manuals/manuals_linux-cluster_jobs/"><meta property="article:section" content="Manuals"><meta property="article:modified_time" content="2021-06-18T16:10:22-07:00"><meta property="og:site_name" content="HPCC Home"><meta itemprop=name content="Linux Cluster - Managing Jobs"><meta itemprop=description content="What is a Job? Submitting and managing jobs is at the heart of using the cluster. A &lsquo;job&rsquo; refers to the script, pipeline or experiment that you run on the nodes in the cluster.
Partitions In the past we used queues under the old Torque system, we now refer to these logically grouped nodes as partitions. There are several different partitions available for cluster users to send jobs to:
 intel  Default partition Nodes: i01-02,i17-i40 Cores: Intel, 256 per user RAM: 1 GB default Time (walltime): 168 hours (7 days) default   batch  Nodes: c01-c48 Cores: AMD, 256 per user RAM: 1 GB default Time (walltime): 168 hours (7 days) default   highmem  Nodes: h01-h06 Cores: Intel, 32 per user RAM: 100 GB min and 1000 GB max Time (walltime): 48 hours (2 days) default   gpu  Nodes: gpu01-gpu05 GPUs: 8 per group RAM: 1 GB default Time (walltime): 48 hours (2 days) default   short  Nodes: Mixed set of nodes from batch, intel, and group partitions Cores: AMD/Intel, 256 per user RAM: 1 GB default Time (walltime): 2 hours Maximum   Group Partition  This partition is unique to the group, if your lab has purchased nodes then you will have a priority partition with the same name as your group (ie."><meta itemprop=dateModified content="2021-06-18T16:10:22-07:00"><meta itemprop=wordCount content="2871"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Linux Cluster - Managing Jobs"><meta name=twitter:description content="What is a Job? Submitting and managing jobs is at the heart of using the cluster. A &lsquo;job&rsquo; refers to the script, pipeline or experiment that you run on the nodes in the cluster.
Partitions In the past we used queues under the old Torque system, we now refer to these logically grouped nodes as partitions. There are several different partitions available for cluster users to send jobs to:
 intel  Default partition Nodes: i01-02,i17-i40 Cores: Intel, 256 per user RAM: 1 GB default Time (walltime): 168 hours (7 days) default   batch  Nodes: c01-c48 Cores: AMD, 256 per user RAM: 1 GB default Time (walltime): 168 hours (7 days) default   highmem  Nodes: h01-h06 Cores: Intel, 32 per user RAM: 100 GB min and 1000 GB max Time (walltime): 48 hours (2 days) default   gpu  Nodes: gpu01-gpu05 GPUs: 8 per group RAM: 1 GB default Time (walltime): 48 hours (2 days) default   short  Nodes: Mixed set of nodes from batch, intel, and group partitions Cores: AMD/Intel, 256 per user RAM: 1 GB default Time (walltime): 2 hours Maximum   Group Partition  This partition is unique to the group, if your lab has purchased nodes then you will have a priority partition with the same name as your group (ie."><link rel=preload href=/scss/main.min.e8d637f30893e85918b2aeae9cd9eab6c68fcd7998810e804d7c9ab542fbfd45.css as=style><link href=/scss/main.min.e8d637f30893e85918b2aeae9cd9eab6c68fcd7998810e804d7c9ab542fbfd45.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://unpkg.com/lunr@2.3.8/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script></head><body class=td-page><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg></span><span class="text-uppercase font-weight-bold">HPCC Home</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/access-rates/><span>Access & Rates</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/contacts/><span>Staff & Contacts</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/related/><span>Related Websites</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/contacts/location/><span>Location</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/documentation/><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/><span>Links</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.29359072432c4e278e68cab0afbecf84.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-default td-outer"><main role=main class=td-main><h2 id=what-is-a-job>What is a Job?</h2><p>Submitting and managing jobs is at the heart of using the cluster. A &lsquo;job&rsquo; refers to the script, pipeline or experiment that you run on the nodes in the cluster.</p><h2 id=partitions>Partitions</h2><p>In the past we used queues under the old Torque system, we now refer to these logically grouped nodes as partitions. There are several different partitions available for cluster users to send jobs to:</p><ul><li>intel<ul><li>Default partition</li><li>Nodes: i01-02,i17-i40</li><li>Cores: Intel, 256 per user</li><li>RAM: 1 GB default</li><li>Time (walltime): 168 hours (7 days) default</li></ul></li><li>batch<ul><li>Nodes: c01-c48</li><li>Cores: AMD, 256 per user</li><li>RAM: 1 GB default</li><li>Time (walltime): 168 hours (7 days) default</li></ul></li><li>highmem<ul><li>Nodes: h01-h06</li><li>Cores: Intel, 32 per user</li><li>RAM: 100 GB min and 1000 GB max</li><li>Time (walltime): 48 hours (2 days) default</li></ul></li><li>gpu<ul><li>Nodes: gpu01-gpu05</li><li>GPUs: 8 per group</li><li>RAM: 1 GB default</li><li>Time (walltime): 48 hours (2 days) default</li></ul></li><li>short<ul><li>Nodes: Mixed set of nodes from batch, intel, and group partitions</li><li>Cores: AMD/Intel, 256 per user</li><li>RAM: 1 GB default</li><li>Time (walltime): 2 hours Maximum</li></ul></li><li>Group Partition<ul><li>This partition is unique to the group, if your lab has purchased nodes then you will have a priority partition with the same name as your group (ie. girkelab).
In order to submit a job to different partitions add the optional &lsquo;-p&rsquo; parameter with the name of the partition you want to use:</li></ul></li></ul><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch -p batch SBATCH_SCRIPT.sh
sbatch -p highmem SBATCH_SCRIPT.sh
sbatch -p gpu SBATCH_SCRIPT.sh
sbatch -p intel SBATCH_SCRIPT.sh
sbatch -p mygroup SBATCH_SCRIPT.sh
</code></pre></div><h2 id=slurm>Slurm</h2><p>Slurm is now our default queuing system across all head nodes. <a href=#getting-started>SSH directly into the cluster</a> and your connection will be automatically load balanced to a head node:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh -XY cluster.hpcc.ucr.edu
</code></pre></div><h3 id=resources-and-limits>Resources and Limits</h3><p>To see your limits you can do the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>slurm_limits
</code></pre></div><p>Check total number of cores used by your group in the all partitions:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>group_cpus
</code></pre></div><p>However this does not tell you when your job will start, since it depends on the duration of each job.
The best way to do this is with the &ldquo;&ndash;start&rdquo; flag on the squeue command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>squeue --start -u <span style=color:#000>$USER</span>
</code></pre></div><h3 id=submitting-jobs>Submitting Jobs</h3><p>There are 2 basic ways to submit jobs; non-interactive, interactive. Slurm will automatically start within the directory where you submitted the job from, so keep that in mind when you use relative file paths.
Non-interactive submission of a SBATCH script:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch SBATCH_SCRIPT.sh
</code></pre></div><p>Here is an example of an SBATCH script:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic>#!/bin/bash -l
</span><span style=color:#8f5902;font-style:italic></span>
<span style=color:#8f5902;font-style:italic>#SBATCH --nodes=1</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --ntasks=1</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=10</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --mem=10G</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --time=1-00:15:00     # 1 day and 15 minutes</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --mail-user=useremail@address.com</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --mail-type=ALL</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --job-name=&#34;just_a_test&#34;</span>
<span style=color:#8f5902;font-style:italic>#SBATCH -p intel # This is the default partition, you can use any of the following; intel, batch, highmem, gpu</span>

<span style=color:#8f5902;font-style:italic># Print current date</span>
date

<span style=color:#8f5902;font-style:italic># Load samtools</span>
module load samtools

<span style=color:#8f5902;font-style:italic># Concatenate BAMs</span>
samtools cat -h header.sam -o out.bam in1.bam in2.bam

<span style=color:#8f5902;font-style:italic># Print name of node</span>
hostname
</code></pre></div><p>The above job will request 1 node, 10 cores (parallel threads), 10GB of memory, for 1 day and 15 minutes. An email will be sent to the user when the status of the job changes (Start, Failed, Completed).
For more information regarding parallel/multi core jobs refer to <a href=#parallelization>Parallelization</a>.</p><p>Interactive submission:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun --pty bash -l
</code></pre></div><p>If you do not specify a partition then the intel partition is used by default.</p><p>Here is a more complete example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun --x11 --mem<span style=color:#ce5c00;font-weight:700>=</span>1gb --cpus-per-task <span style=color:#0000cf;font-weight:700>1</span> --ntasks <span style=color:#0000cf;font-weight:700>1</span> --time 10:00:00 --pty bash -l
</code></pre></div><p>The above example enables X11 forwarding and requests, 1GB of memory, 1 cores, for 10 hours within an interactive session.</p><h3 id=monitoring-jobs>Monitoring Jobs</h3><p>To check on your jobs states, run the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>squeue -u <span style=color:#000>$USER</span> --start
</code></pre></div><p>To list all the details of a specific job, run the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scontrol show job JOBID
</code></pre></div><p>To view past jobs and their details, run the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sacct -u <span style=color:#000>$USER</span> -l
</code></pre></div><p>You can also adjust the start <code>-S</code> time and/or end <code>-E</code> time to view, using the YYYY-MM-DD format.
For example, the following command uses start and end times:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sacct -u <span style=color:#000>$USER</span> -S 2018-01-01 -E 2018-08-30 -l <span style=color:#000;font-weight:700>|</span> less -S <span style=color:#8f5902;font-style:italic># Type &#39;q&#39; to quit</span>
</code></pre></div><h3 id=canceling-jobs>Canceling Jobs</h3><p>In cancel/stop your job run the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scancel &lt;JOBID&gt;
</code></pre></div><p>You can also cancel multiple jobs:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>scancel &lt;JOBID1&gt; &lt;JOBID2&gt; &lt;JOBID3&gt;
</code></pre></div><p>If you want to cancel/stop/kill ALL your jobs it is possible with the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic># Be very careful when running this, it will kill all your jobs.</span>
squeue --user <span style=color:#000>$USER</span> --noheader --format <span style=color:#4e9a06>&#39;%i&#39;</span> <span style=color:#000;font-weight:700>|</span> xargs scancel
</code></pre></div><p>For more information please refer to <a href=https://slurm.schedmd.com/scancel.html title="Slurm scancel doc">Slurm scancel documentation</a>.</p><h3 id=advanced-jobs>Advanced Jobs</h3><p>There is a third way of submitting jobs by using steps.
Single Step submission:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun &lt;command&gt;
</code></pre></div><p>Under a single step job your command will hang until appropriate resources are found and when the step command is finished the results will be sent back on STDOUT. This may take some time depending on the job load of the cluster.
Multi Step submission:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>salloc -N <span style=color:#0000cf;font-weight:700>4</span> bash -l
srun &lt;command&gt;
...
srun &lt;command&gt;
<span style=color:#204a87>exit</span>
</code></pre></div><p>Under a multi step job the salloc command will request resources and then your parent shell will be running on the head node. This means that all commands will be executed on the head node unless preceeded by the srun command. You will also need to exit this shell in order to terminate your job.</p><h3 id=highmem-jobs>Highmem Jobs</h3><p>The highmem partition does not have a default amount of memory set, however it does has a minimum limit of 100GB per job. This means that you need to explicity request at least 100GB or more of memory.</p><p>Non-Interactive:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch -p highmem --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>24:00:00 SBATCH_SCRIPT.sh
</code></pre></div><p>Interactive</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun -p highmem --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>24:00:00 --pty bash -l
</code></pre></div><p>Of course you should adjust the time argument according to your job requirements.</p><h3 id=gpu-jobs>GPU Jobs</h3><p>GPU nodes have multiple GPUs, and very in type (K80 or P100). This means you need to request how many GPUs and of what type that you would like to use.</p><p>To request a gpu of any type, only indicate how many GPUs you would like to use.</p><p>Non-Interactive:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:1 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 SBATCH_SCRIPT.sh
</code></pre></div><p>Interactive</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:4 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 --pty bash -l
</code></pre></div><p>Since the HPCC Cluster has two types of GPUs installed (K80s and P100s), GPUs can be requested explicitly by type.</p><p>Non-Interactive:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:k80:1 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 SBATCH_SCRIPT.sh
sbatch -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:p100:1 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 SBATCH_SCRIPT.sh
</code></pre></div><p>Interactive</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:k80:1 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 --pty bash -l
srun -p gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:p100:1 --mem<span style=color:#ce5c00;font-weight:700>=</span>100g --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 --pty bash -l
</code></pre></div><p>Of course you should adjust the time argument according to your job requirements.</p><p>Once your job starts your code must reference the environment variable &ldquo;CUDA_VISIBLE_DEVICES&rdquo; which will indicate which GPUs have been assigned to your job. Most CUDA enabled software, like MegaHIT, will check this environment variable and automatically limit accordingly.</p><p>For example, when reserving 4 GPUs for a NAMD2 job:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#204a87>echo</span> <span style=color:#000>$CUDA_VISIBLE_DEVICES</span>
0,1,2,3
namd2 +idlepoll +devices <span style=color:#000>$CUDA_VISIBLE_DEVICES</span> MD1.namd
</code></pre></div><p>Each group is limited to a maximum of 8 GPUs on the gpu partition. Please be respectful of others and keep in mind that the GPU nodes are a limited shared resource.
Since the CUDA libraries will only run with GPU hardward, development and compiling of code must be done within a job session on a GPU node.</p><p>Here are a few more examples of jobs that utilize more complex features (ie. array, dependency, MPI etc):
<a href=https://github.com/ucr-hpcc/hpcc_slurm_examples>Slurm Examples</a></p><h3 id=web-browser-access>Web Browser Access</h3><h4 id=ports>Ports</h4><p>Some jobs require web browser access in order to utilize the software effectively.
These kinds of jobs typically use (bind) ports in order to provide a graphical user interface (GUI) through a web browser.
Users are able to run jobs that use (bind) ports on a compute node.
Any port can be used on any compute node, as long as the port number is greater than 1000 and it is not already in use (bound).</p><h4 id=tunneling>Tunneling</h4><p>Once a job is running on a compute node and bound to a port, you may access this compute node via a web browser.
This is accomplished by using 2 chained SSH tunnels to route traffic through our firewall.
This acts much like 2 runners in a relay race, handing the baton to the next runer, to get past a security checkpoint.</p><p>We will create a tunnel that goes though a headnode and connect to a compute node on a particular port:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh -NL 8888:NodeName:8888 username@cluster.hpcc.ucr.edu
</code></pre></div><p>Port 8888 (first) is the local port you will be using on your laptop.
NodeName is the compute node where where job is running, which can be found by using the <code>squeue -u $USER</code> command.
Port 8888 (second) is the remote port on the compute node.
Again, the NodeName and ports will be different depending on where your job runs and what port your job uses.</p><p>At this point you may need to provide a password to make the SSH tunnel.
Once this has succeeded, the command will hang (this is normal).
Leave this session connected, if you close it your tunnel will be closed.</p><p>Then open a browser on your local computer (PC/laptop) and point it to:</p><pre><code>http://localhost:8888
</code></pre><p>If your job uses TSL/SSL, so you may need to try https if the above does not work:</p><pre><code>https://localhost:8888
</code></pre><h4 id=examples>Examples</h4><p>A perfect example of this method is used for Jupyter Lab/Notebook.
For more details please refer to the following <a href=https://github.com/ucr-hpcc/hpcc_slurm_examples/tree/master/jupyter>Jupyter Example</a>.</p><h3 id=desktop-environments>Desktop Environments</h3><h4 id=vnc-server-cluster>VNC Server (cluster)</h4><p><strong>Start VNC Server</strong></p><p>Log into the cluster:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh username@cluster.hpcc.ucr.edu
</code></pre></div><p>The first time you run the vncserver it will need to be configured:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>vncserver -fg
</code></pre></div><p>You should set a password for yourself, and the read-only password is optional.</p><p>Then configure X Startup with the following command:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#39;/usr/bin/ssh-agent /usr/bin/dbus-launch --exit-with-session /usr/bin/gnome-session --session=gnome-classic&#39;</span> &gt; /rhome/<span style=color:#000>$USER</span>/.vnc/xstartup
</code></pre></div><p>After your vncserver is configured, submit a vncserver job to get it started:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch -p short,batch --cpus-per-task<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>4</span> --mem<span style=color:#ce5c00;font-weight:700>=</span>10g --time<span style=color:#ce5c00;font-weight:700>=</span>2:00:00 --wrap<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#39;vncserver -fg&#39;</span> --output<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#39;vncserver-%j.out&#39;</span>
</code></pre></div><blockquote><p>Note: Appropriate job resources should be requested based on the processes you will be running from within the VNC session.</p></blockquote><p>Check the contents of your job log to determine the <code>NodeName</code> and <code>Port</code> you were assigned:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>cat vncserver-*.out
</code></pre></div><p>The contents of your slurm job log should be similar to the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>vncserver

New <span style=color:#4e9a06>&#39;i54:1&#39;</span> desktop is i54:1

Creating default startup script /rhome/username/.vnc/xstartup
Starting applications specified in /rhome/username/.vnc/xstartup
Log file is /rhome/username/.vnc/i54:1.log
</code></pre></div><p>The VNC <code>Port</code> used should be 5900+N, N being the display number mentioned above in the format <code>NodeName</code>:<code>DisplayNumber</code> (ie. <code>i54:1</code>).
In this example (default), the port is <code>5901</code>, if this <code>Port</code> were already in use then the vncserver will automatically increment the DisplayNumber and you might find something like <code>i54:2</code> or <code>i54:3</code> and so on.</p><p><strong>Stop VNC Server</strong></p><p>To stop the vncserver, you can click on the logout option from the upper right hand menu from within your VNC desktop environment.
If you want to kill your vncserver manually, then you will need to do the following:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh NodeName <span style=color:#4e9a06>&#39;vncserver -kill :DisplayNumber&#39;</span>
</code></pre></div><p>You will need to replace <code>NodeName</code> with the node name of your where your job is running, and the <code>DisplayNumber</code> with the DisplayNumber from your slurm job log.</p><h4 id=vnc-client-desktoplaptop>VNC Client (Desktop/Laptop)</h4><p>After you know the <code>NodeName</code> and VNC <code>Port</code> you should be able to create an SSH tunnel to your vncserver, like so:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh -N -L Port:NodeName:Port cluster.hpcc.ucr.edu
</code></pre></div><p>Now let us create an SSH tunnel on your local machine (desktop/laptop) using the <code>NodeName</code> and VNC <code>Port</code> from above:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh -L 5901:i54:5901 cluster.hpcc.ucr.edu
</code></pre></div><p>After you have logged into the cluster with this shell, log into the node where your VNC server is running:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>ssh NodeName
</code></pre></div><p>After you have logged into the correct <code>NodeName</code>, just let this terminal sit here, do not close it.</p><p>Then launch vncviewer on your local system (laptop/workstation), like so:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>vncviewer localhost:5901
</code></pre></div><p>After launching the vncviewer, and providing your VNC password (not your cluster password), you should be able to see a Linux desktop environment.</p><p>For more information regarding tunnels and VNC in MS Windows, please refer <a href=https://docs.ycrc.yale.edu/clusters-at-yale/access/vnc/>More VNC Info</a>.</p><h3 id=licenses>Licenses</h3><p>The cluster currently supports <a href=software_commercial>Commercial Software</a>. Since most of the licenses are campus wide there is no need to track individual jobs. One exception is the Intel Parallel Suite, which contains the Intel compilers.</p><p>The <code>--licenses</code> flag is used to request a license for Intel compilers, for example:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun --license<span style=color:#ce5c00;font-weight:700>=</span>intel:1 -p short --mem<span style=color:#ce5c00;font-weight:700>=</span>10g --cpus-per-task<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>10</span> --time<span style=color:#ce5c00;font-weight:700>=</span>2:00:00 --pty bash -l
module load intel
icc -help
</code></pre></div><p>The above interactive submission will request 1 Intel license, 10GB of RAM, 10 CPU cores for 2 hours on the short partition.
The short parititon can only be used for a maximum of 2 hours, however for compilation this could be sufficient.
It is recommended that you separate your compilation job from your computation/analysis job.
This way you will only have the license checked out for the duration of compilation, and not the during the execution of the analysis.</p><h2 id=parallelization>Parallelization</h2><p>There are 3 major ways to parallelize work on the cluster:</p><ol><li>Batch</li><li>Thread</li><li>MPI</li></ol><h3 id=parallel-methods>Parallel Methods</h3><p>For <strong>batch</strong> jobs, all that is required is that you have a way to split up the data and submit multiple jobs running with the different chunks.
Some data sets, for example a FASTA file is very easy to split up (ie. fasta-splitter). This can also be more easily achieved by submitting an array job. For more details please refer to <a href=#advanced-jobs>Advanced Jobs</a>.</p><p>For <strong>threaded</strong> jobs, your software must have an option referring to &ldquo;number of threads&rdquo; or &ldquo;number of processors&rdquo;. Once the thread/processor option is identified in the software, (ie. blastn flag <code>-num_threads 4</code>) you can use that as long as you also request the same number of CPU cores (ie. slurm flag <code>--cpus-per-task=4</code>).</p><p>For <strong>MPI</strong> jobs, your software must be MPI enabled. This generally means that it was compiled with MPI libraries. Please refer to the user manual of the software you wish to use as well as our documentation regarding <a href=#mpi>MPI</a>. It is important that the number of cores used is equal to the number requested.</p><p>In Slurm you will need 2 different flags to request cores, which may seem similar, however they have different purposes:</p><ul><li>The <code>--cpus-per-task=N</code> will provide N number of virtual cores with locality as a factor.
Closer virtual cores can be faster, assuming there is a need for rapid communication between threads.
Generally, this is good for threading, however not so good for independent subprocesses nor for MPI.</li><li>The <code>--ntasks=N</code> flag will provide N number of physical cores on a single or even multiple nodes.
These cores can be further away, since the need for physical CPUs and dedicated memory is more important.
Generally this is good for independent subprocesses, and MPI, however not so good for threading.</li></ul><p>Here is a table to better explain when to use these Slurm options:</p><p>| | Single Threaded | Multi Threaded (OpenMP) | MPI only | MPI + Multi Threaded (hybrid) |</p><table><thead><tr><th>Slurm Flag</th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th></tr></thead><tbody><tr><td><code>--cpus-per-task</code></td><td style=text-align:center></td><td style=text-align:center>X</td><td style=text-align:center></td><td style=text-align:center>X</td></tr><tr><td><code>--ntasks</code></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>X</td><td style=text-align:center>X</td></tr></tbody></table><p>As you can see:</p><ol><li>A single threaded job would use neither Slurm option, since Slurm already assumes at least a single core.</li><li>A multi threaded OpenMP job would use <code>--cpus-per-task</code>.</li><li>A MPI job would use <code>--ntasks</code>.</li><li>A Hybrid job would use both.</li></ol><p>For more details on how these Slurm options work please review <a href=https://slurm.schedmd.com/mc_support.html>Slurm Multi-core/Multi-thread Support</a>.</p><h4 id=mpi>MPI</h4><p>MPI stands for the Message Passing Interface. MPI is a standardized API typically used for parallel and/or distributed computing.
The HPCC cluster has a custom compiled versions of MPI that allows users to run MPI jobs across multiple nodes.
These types of jobs have the ability to take advantage of hundreds of CPU cores symultaniously, thus improving compute time.</p><p>Many implementations of MPI exists, however we only support the following:</p><ul><li><a href=http://www.open-mpi.org/>Open MPI</a></li><li><a href=http://www.mpich.org/>MPICH</a></li><li><a href=https://software.intel.com/en-us/mpi-developer-guide-linux>IMPI</a></li></ul><p>For general information on MPI under Slurm look <a href=https://slurm.schedmd.com/mpi_guide.html>here</a>.
If you need to compile an MPI application then please email <a href=mailto:support@hpcc.ucr.edu>support@hpcc.ucr.edu</a> for assistance.</p><p>When submitting MPI jobs it is best to ensure that the nodes are identical, since MPI is sensitive to differences in CPU and/or memory speeds.
The <code>batch</code> and <code>intel</code> partitions are designed to be homogeneous, however, the <code>short</code> partition is a mixed set of nodes.
When using the <code>short</code> partition for MPI append the constraint flag for Slurm.</p><p><strong>Short Example</strong></p><p>Here is an example that shows how to ensure that your job will only run on <code>intel</code> nodes from the <code>short</code> partition:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch -p short --constraint<span style=color:#ce5c00;font-weight:700>=</span>intel myJobScript.sh
</code></pre></div><p><strong>NAMD Example</strong></p><p>To run a NAMD2 process as an OpenMPI job on the cluster:</p><ol><li><p>Log-in to the cluster</p></li><li><p>Create SBATCH script</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic>#!/bin/bash -l
</span><span style=color:#8f5902;font-style:italic></span>
<span style=color:#8f5902;font-style:italic>#SBATCH -J c3d_cr2_md</span>
<span style=color:#8f5902;font-style:italic>#SBATCH -p batch</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --ntasks=32</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --mem=16gb</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --time=01:00:00</span>

<span style=color:#8f5902;font-style:italic># Load needed modules</span>
<span style=color:#8f5902;font-style:italic># You could also load frequently used modules from within your ~/.bashrc</span>
module load slurm <span style=color:#8f5902;font-style:italic># Should already be loaded</span>
module load openmpi <span style=color:#8f5902;font-style:italic># Should already be loaded</span>
module load namd

<span style=color:#8f5902;font-style:italic># Run job utilizing all requested processors</span>
<span style=color:#8f5902;font-style:italic># Please visit the namd site for usage details: http://www.ks.uiuc.edu/Research/namd/</span>
mpirun --mca btl ^tcp namd2 run.conf <span style=color:#000;font-weight:700>&amp;</span>&gt; run_namd.log
</code></pre></div></li><li><p>Submit SBATCH script to Slurm queuing system</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sbatch run_namd.sh
</code></pre></div></li></ol><p><strong>Maker Example</strong></p><p>OpenMPI does not function properly with Maker, you must use MPICH.
Our version of MPICH does not use the mpirun/mpiexec wrappers, instead use srun:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#8f5902;font-style:italic>#!/bin/bash -l
</span><span style=color:#8f5902;font-style:italic></span>
<span style=color:#8f5902;font-style:italic>#SBATCH -p intel</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --ntasks=32</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --mem=16gb</span>
<span style=color:#8f5902;font-style:italic>#SBATCH --time=01:00:00</span>

<span style=color:#8f5902;font-style:italic># Load maker</span>
module load maker/2.31.11

mpirun maker <span style=color:#8f5902;font-style:italic># Provide appropriate maker options here</span>

</code></pre></div><h2 id=more-examples>More examples</h2><p>The range of differing jobs and how to submit them is endless:</p><pre><code>1. Singularity containers
2. Database services
3. Graphical user interfaces
4. Etc ...
</code></pre><p>For a growing list of examples please visit <a href=https://github.com/ucr-hpcc/hpcc_slurm_examples>HPCC Slurm Examples</a>.</p></main><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Immediate Email Support" aria-label="Immediate Email Support"><a class=text-white target=_blank rel="noopener noreferrer" href=support@hpcc.ucr.edu><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank rel="noopener noreferrer" href=https://twitter.com/UCR_HPCC><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank rel="noopener noreferrer" href=https://ucr-hpcc.slack.com/><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title aria-label><a class=text-white target=_blank rel="noopener noreferrer" href><i></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title aria-label><a class=text-white target=_blank rel="noopener noreferrer" href><i></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title aria-label><a class=text-white target=_blank rel="noopener noreferrer" href><i></i></a></li></ul></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2021 The Docsy Authors All Rights Reserved</small>
<small class=ml-1><a href=https://policies.google.com/privacy target=_blank>Privacy Policy</a></small><p class=mt-2><a href=/about/>About</a></p></div></div></div></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js integrity=sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49 crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js integrity=sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy crossorigin=anonymous></script><script src=/js/main.min.63db3e552bd0543be17ce4a79a18b744e9cb72256bec42b540e3ab0cd43722d0.js integrity="sha256-Y9s+VSvQVDvhfOSnmhi3ROnLciVr7EK1QOOrDNQ3ItA=" crossorigin=anonymous></script></body></html>